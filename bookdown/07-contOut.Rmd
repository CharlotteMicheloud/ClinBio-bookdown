# Analysis of continuous outcomes {#contOut}



Effect size estimates quantify clinical relevance, confidence intervals 
indicate the (im)precision of the effect size estimates as population 
values, and $P$-values quantify statistical significance, i.e. the 
evidence against the null hypothesis. 
All three should be reported for each outcome of an RCT 
and the reported $P$-value should be compatible with the selected confidence 
interval. 
Binary outcomes are discussed in Chapter \@ref(binOut). 

:::{.example #didgeridoo}
The Didgeridoo study [@puhan]
is a randomized controlled trial with simple randomization. Patients 
with moderate obstructive sleep apnoea syndrome have been randomized to 
4 months of Didgeridoo practice ($m = 14$) or 4 months on the waiting list
($n = 11$).
The primary endpoint is the Epworth scale (integers from 0-24). This scale is 
ordinal but for the analysis, it is considered continuous due to the 
large number of possible values. Measurements are taken at the start of the 
study (*Baseline*) and after four months (*Follow-up*). 
Figure \@ref(fig:puhanFUmeas) compares the follow-up measurements of the 
treatment and control groups for the primary endpoint. 
:::

```{r puhan, echo=FALSE, fig.cap="Abstract of publication of the Didgeridoo Study.", fig.align='center', eval = FALSE}
knitr::include_graphics("figures/abstract2.pdf")
```


```{r echo=F}
library(xtable)
X <- read.table("data/alphorn.dat", header=TRUE,sep="\t")
X$treatment <- abs(X$group-1)
X$treatment <- ifelse(X$treatment==0, "Control", "Didgeridoo") 
X$treatment <- factor(X$treatment, labels=c("Control", "Didgeridoo"))
##X$treatment <- as.factor(X$treatment, levels=c("Didgeridoo", "Control"))
attach(X, warn.conflicts = FALSE)
x1<-X[group==0,17:18]
y1<-X[group==1,17:18]
names<-c("didgeridoo","control")
## treatment <- abs(group-1)
epworth <- rbind(x1, y1)
baseline <- epworth[,1]
```

```{r echo = TRUE, eval = T}
table(treatment)
```

```{r puhanFUmeas, fig.cap = "Follow-up measurements of primary endpoint in the Didgeridoo Study.", echo=F}
ylab<-c("Epworth scale")
ylim1<-c(0,24)
nx<-dim(x1)[1]
ny<-dim(y1)[1]
histborder <- "black"#"white"
histcol <- gray(0.4)
pch.mean <- 8 # 4, 16
pch.median <- 17
lwd.lines <- 2
lwds <- 1.5 ## points 
pars.boxplot <- list(boxwex = 0.5, staplewex = 0.5, medlwd = 2, whisklty = 1,  whisklwd = 1)

par(mfrow=c(1,1))
library(beeswarm)

beeswarm(epworth2 ~ group, data = X, method = "center", ylab="Epworth-Scale at Follow-up", xlab="Treatment group", pch=16, cex=1.5, xlim = c(0.5, 2.5), xaxt = "n", las = 1, lwd = lwds, main = "", col="red")
axis(1, c(1, 2), (c("Didgeridoo", "Control")), padj = 0.5)

eps <- 0.1
names <- c("Didgeridoo", "Control")
mymeans <- numeric()
for(i in 1:2){
    j <- names[i]
    mymeans[i] <- mean(X$epworth2[X$group==(i-1)])
    mymean <- mymeans[i]
    lines(c(i-eps, i+eps), rep(mymean, 2), col = "black", lwd = 3)
    text(i-eps, mymean, as.character(round(mymean, 2)), pos=2)
}

diffmean <- mymeans[1]-mymeans[2]

text(1.25, 15, paste0("Mean difference: ", as.character(round(diffmean, 2))))

legend("topleft", "Mean", col = "black", lwd = 3, bty = "n", cex=1)



f.up <- epworth[,2]

``` 

## Comparison of follow-up  measurements
### $t$-test


In order to compare the follow-up measurements between the two 
groups, a $t$-test can be performed.
In a $t$-test, data are assumed to be normally distributed, 
and the measurements in the two groups to be independent: 
with mean $\mu_T$, variance $\sigma^2_T$ and sample size $m$
in the treatment group, and with mean 
 $\mu_C$, variance $\sigma^2_C$ and sample size $n$
in the control group.
The quantity of interest is the mean difference $\Delta = \mu_T - \mu_C$. The variances are assumed to be equal in the two groups, 
i.e. $\sigma^2_T = \sigma^2_C = \sigma^2$. 

The null hypothesis of a $t$-test is 
$$
H_0: \Delta = 0 .
$$
The estimate $\widehat\Delta$ of $\Delta$ is the 
difference in sample means. 
The $t$-test statistic is 
$$
T = \frac{\widehat\Delta}{\se(\widehat\Delta)}, 
$$
with 
$$
\se(\widehat\Delta) = s \cdot \sqrt{\frac{1}{m} + \frac{1}{n}}, 
$$
where $s^2$ is the pooled estimate of the variance $\sigma^2$:
$$
s^2 = \frac{(m - 1)s^2_T + (n-1)s^2_C}{m + n - 2}.
$$
Here $s^2_T$ and $s^2_C$ are the sample variances in the two groups.
Under the null hypothesis of no effect, the test statistic 
T follows a $t$-distribution with $m + n - 2$ degrees of freedom (df). 
For large degrees of freedom, the $t$-distribution is close to a standard normal distribution,  as illustrated in Figure \@ref(fig:tdistr).


```{r tdistr, fig.cap = "Comparison of $t$-distribution (with large degree of freedom) to a standard normal distribution.", echo = FALSE}
df <- 23
grid <- seq(-4, 4, .01)
tDensity <- dt(grid, df=df)
normDensity <- dnorm(grid)
par(las=1)
matplot(grid, cbind(normDensity, tDensity), type="l", lty=c(2, 1), lwd=2, xlab="x", ylab="density")
legend("topright", lty=c(2, 1), lwd=2, col=c(1,2), legend=c("standard normal", "t with 23 df"))
``` 
Let us now apply the $t$-test to the follow-up measurements in
Example \@ref(exm:didgeridoo).
```{r echo=T}
# t-test
print(tTest1 <- t.test(f.up ~ treatment, var.equal=TRUE))
(DifferenceInMeans <- mean(tTest1$conf.int))  
```
There is no evidence for a difference in follow-up 
means ($P$-value = `r biostatUZH::formatPval(tTest1$p.value)`). 
The $t$-test gives identical results as a linear regression analysis:
```{r echo=T, results="asis"}
# regression analysis
library(biostatUZH)
model1 <- lm(f.up ~ treatment)
knitr::kable(tableRegression(model1, intercept=FALSE, 
                             latex = FALSE, xtable = FALSE))
```
The advantages of the regression analysis are that it can easily be 
generalized and that the residuals can be checked.
In the Didgeridoo study, the regression diagnostics indicate a poor 
model fit, with signs of variance heterogeneity: 

```{r, echo = TRUE}
par(mfrow=c(1,2), pty="s", las=1)
plot(model1, which=1, pch=19, col=treatment)
plot(model1, which=2, pch=19, col=treatment)
```



Bartlett's test can be used to test the equality of variance, 
using the `R` function `bartlett.test()`: 

```{r}
print(bTest <- bartlett.test(f.up ~ treatment))
```

The test indicates strong evidence for variance heterogeneity ($p=
0.003$), which confirms the earlier findings based on regression
diagnostics.  However, it is not recommended to pre-test for equal
variances and then choose between a $t$-test or Welch's test. Such a
two-stage procedure fails to control the Type-I error rate and usually
makes the situation worse, see @Zimmerman2004.


### Welch's test

In the case of unequal variances, $\sigma_T^2$ and $\sigma^2_C$ are assumed to be
different and the standard error of $\widehat\Delta$ then is

\begin{equation*}
  \SE(\widehat \Delta) = \sqrt{\frac{s_T^2}{m} + \frac{s_C^2}{n}},
\end{equation*}
where $s_T^2$ and $s_C^2$ are estimates of the variances $\sigma_T^2$ and
$\sigma_C^2$ in the two groups. In this case, the exact null distribution of
$T=\widehat \Delta/{\SE(\widehat \Delta)}$ is unknown. 
Welch's test is an appropriate solution, and the default in `t.test()`, 
since it is not recommended to pre-test for equal variances and then choose between a t-test or Welchâ€™s test (two-stage procedure).
This test can have non-integer degrees of freedom. 

<!-- Approximate solutions include -->

<!-- -  *Welch's Test*, which uses a $t$-distribution with (non-integer) degrees of freedom. -->
<!-- -  *Behrens Test*, implemented in `biostatUZH::behrensTest()`. -->
<!-- -  The *Mann-Whitney Test* which is a nonparametric alternative -->
<!-- - The permutation test, based on a randomization model, see @matthews  -->
<!-- (Section 7.6) and @Ludbrook1998.  -->

<!-- With all these methods, adjustments for covariates are not standard.  -->



```{r echo = TRUE, warning = FALSE}
## Welch Test
print(WelchTest1 <- t.test(f.up ~ treatment))

```

```{r, eval = FALSE, echo = FALSE}


## Behrens Test
library(biostatUZH)
behrens.test(f.up ~ treatment)

## Mann-Whitney Test
```

There is no evidence for a difference in follow-up means ($p = 0.26$)


## Analysis of baseline and follow-up measurements {#sec:baseline}

In the previous section, we focused solely on follow-up measurements. 
Now, we consider both baseline and follow-up measurements in the analysis.

### Change scores

Baseline values may be imbalanced between treatment groups just as any other
prognostic factor. To analyse change from baseline, we use change scores:

:::{.definition}
The *change score* is the change from baseline defined as:
  \[
  \mbox{change score} = \mbox{follow-up} - \mbox{baseline}.
  \]
:::

:::{.example #didgeridoo name="continued"}
Figure \@ref(fig:puhanBaselineFU) shows the combinations of baseline and
follow-up measurements for each individual. 
It is visible that the change from baseline to follow-up is larger in the 
treatment group than in the control group. Figure \@ref(fig:puhanChangeScore)
now directly compares the change scores.
:::

```{r puhanBaselineFU, fig.cap = "Individual baseline and follow-up measurements in the Didgeridoo Study by treatment group.", echo=F}
ylab<-c("Epworth scale")
ylim1<-c(0,24)
nx<-dim(x1)[1]
ny<-dim(y1)[1]

par(mfrow=c(1,2), las=1)

plot(0,0,type="n",xlim=c(0,1),ylim=ylim1,xlab="",xaxt="n",ylab=ylab[1])
points(rep(0,nx),x1[,1],pch=19)
points(rep(1,nx),x1[,2],pch=19)
for(i in 1:nx) lines(c(0,1),c(x1[i,1],x1[i,2]),lty=1,col=5)
title(names[1],cex=0.8)

plot(0,0,type="n",xlim=c(0,1),ylim=ylim1,xlab="",xaxt="n",ylab=ylab[1])
points(rep(0,ny),y1[,1],pch=19)
points(rep(1,ny),y1[,2],pch=19)
for(i in 1:ny) lines(c(0,1),c(y1[i,1],y1[i,2]),lty=1,col=1)
title(names[2],cex=0.8)
```



```{r puhanChangeScore, fig.cap = "Change scores for primary endpoint in the Didgeridoo Study.", echo=F}
ylab<-c("Epworth scale")
ylim1<-c(0,24)
nx<-dim(x1)[1]
ny<-dim(y1)[1]
histborder <- "black"#"white"
histcol <- gray(0.4)
pch.mean <- 8 # 4, 16
pch.median <- 17
lwd.lines <- 2
lwds <- 1.5 ## points 
pars.boxplot <- list(boxwex = 0.5, staplewex = 0.5, medlwd = 2, whisklty = 1,  whisklwd = 1)

par(mfrow=c(1,1))
library(beeswarm)
change.score <- f.up - baseline

beeswarm(change.score ~ group, data = X, method = "center", ylab="Epworth Change Score", xlab="Group", pch=16, cex=1.5, xlim = c(0.5, 2.5), ylim=c(-10, 5), xaxt = "n", las = 1, lwd = lwds, main = "", col="red")
axis(1, c(1, 2), (c("Didgeridoo", "Control")), padj = 0.5)
abline(0, 0, lty=2)

eps <- 0.1
names <- c("Didgeridoo", "Control")
mymeans <- numeric()
for(i in 1:2){
    j <- names[i]
    mymeans[i] <- mean(change.score[X$group==(i-1)])
    mymean <- mymeans[i]
    lines(c(i-eps, i+eps), rep(mymean, 2), col = "black", lwd = 3)
    text(i-eps, mymean, as.character(round(mymean, 2)), pos=2)
}

diffmean <- mymeans[1]-mymeans[2]

text(1.25, 2, paste0("Mean difference: ", as.character(round(diffmean, 2))))

legend("topleft", "Mean", col = "black", lwd = 3, bty = "n", cex=1)

f.up <- epworth[,2]

```


A change score analysis for the Didgeridoo study using a $t$-test yields:

```{r echo=T}
change.score <- f.up - baseline
print(tTest2 <- t.test(change.score ~ treatment, var.equal=TRUE))
(DifferenceInMeans <- mean(tTest2$conf.int))  
```

There is hence evidence for a difference in mean change score 
between the two groups ($p = `r biostatUZH::formatPval(tTest2$p.value)`$).

The change score analysis can also be done with a regression model: 

```{r, echo = TRUE}
# Change score analysis
model2 <- lm(f.up ~ treatment + offset(baseline))
knitr::kable(tableRegression(model2, intercept=FALSE,
                             latex = FALSE, xtable = FALSE))
```

The `offset(x)` command fixes the coefficient of `x` at 1.

The regression diagnostics show a somewhat better model fit: 
```{r}
par(mfrow=c(1,2), pty="s", las=1)
plot(model2, which=1, pch=19, col=treatment, add.smooth=FALSE)
plot(model2, which=2, pch=19, col=treatment)
```

As a sensitivity analysis, other tests can be applied: 

- Welch's test,
- Behren's test, using `biostatUZH::behrens.test()`, which 
can be derived with Bayesian arguments,
- Mann Whitney test, a nonparametric alternative,
- Permutation test, which follows the randomization model approach. 

Adjustments for covariates are not standard with all these methods. 

#### Welch's and Behrens' tests {-}
Both Welch's and Behren's test 
indicate evidence for a difference in mean change score:

```{r, echo = TRUE}
(WelchTest2 <- t.test(change.score ~ treatment, var.equal=FALSE))
(BehrensTest2 <- behrens.test(change.score ~ treatment))
```

#### Mann-Whitney test {-}
Mann-Whitney test gives a confidence for the median of the 
difference between a sample from the Didgeridoo and the control 
group (`difference in location`), which is hard to interpret: 

```{r, echo = TRUE, warning = FALSE}
(MWTest2 <- wilcox.test(change.score ~ treatment, conf.int=TRUE))
```

Mann-Whitney test has less assumptions that the $t$-test, but 
also less power. 

#### Permutation test {-} 
An alternative method to compute a $p$-value is the permutation test based on 
the randomisation model. The idea is that, under the null hypothesis 
$H_0$, the mean difference does not 
depend on the treatment allocation. The distribution of the mean difference 
under $H_0$ is derived under all possible permutations of treatment 
allocation. The comparison with the observed difference then 
gives a $p$-value. In practice, a Monte Carlo random sample is 
taken from all possible permutations. This approach can be extended to 
stratified randomisation, etc. Figure \@ref(fig:permut) illustrated the 
permutation test for the change score analysis. Not that in total, 
there are ${25 \choose 14} \approx `r round(choose(25, 14)/1000000, 1)`$ 
Mio. distinct permutations of treatment allocation.

```{r permut, echo = FALSE, fig.cap = "Mean difference in change score analysis based on 10000 random permutations"}

alphorn   <- read.csv("data/alphorn.csv", sep = ",", as.is = TRUE)
alphorn$group_alp <- factor(alphorn$group, levels = 0:1, labels = c("Didgeridoo", "Kontrollgruppe"))
alphorn$sex_f <- factor(alphorn$gender, levels = 0:1, labels = c("Mann", "Frau"))
dat <- alphorn


data1 <- alphorn[alphorn$group_alp == "Didgeridoo", "change_e"]
data2 <- alphorn[alphorn$group_alp == "Kontrollgruppe", "change_e"]
d <- c(data1, data2)

n1 <- length(data1)
n2 <- length(data2)
n <- n1 + n2

diff <- function(d1, d2){
  return(mean(d1, na.rm = TRUE) - mean(d2, na.rm = TRUE))
}

obs.diff <- diff(data1, data2)
niter <- 9999
diff.permut <- rep(NA, niter)
set.seed(03072008)

for(i in 1:niter){
 s <- sample(n, n1)
 diff.permut[i] <- diff(d[s], d[-s])
}

xm <- ceiling(max(-min(diff.permut),max(diff.permut)))
b <- seq(-xm, xm, by = .5)
truehist(diff.permut, xlab="Mean difference", breaks=b, col="white", border = histborder, las = 1, cex.lab=1.5, cex.axis=1.5)
T <- obs.diff

## rechts von -T
colarea <- colpal4[3]
ind <- (b >= -T) * c(1:length(b))
ind <- ind > 0
h <- hist(diff.permut, breaks = b, plot = FALSE)
rect(b[ind], 0, c(b[ind][-1], max(b) + 1), h$density[ind], col = colarea)
rect(-T, 0, min(b[b > -T]), rev(h$density[b <= T])[1], col = colarea)
lines(c(-T, -T), c(0,0.09), col = colpal4[1], lwd=lwd.lines + 2, lty =1)

## links von T
ind <- (b <= T) * c(1:length(b))
ind <- ind > 0
rect(b[ind], 0, c(b[ind][-1], T), h$density[ind][1:(length(ind) - 1)], col = colarea)
lines(c(T, T), c(0,0.09), col = colpal4[1], lwd=lwd.lines + 2, lty =1)
text(obs.diff-0.9, 0.12, adj=0, paste("Observed difference \n = ", as.character(format(obs.diff, nsmall=2, digits=2)), sep=""), col=colpal4[1], cex=1.2)
text(-obs.diff-0.3, 0.1125, adj=0, paste("", as.character(format(-obs.diff, nsmall=2, digits=2)), sep=""), col=colpal4[1], cex=1.2)
pPermute <- mean(abs(obs.diff)<abs(diff.permut))
text(3, 0.2, paste("two-sided\np = ", as.character(round(pPermute,3)), sep=""), cex=1.2, adj = 0, font = 2, col=colarea)
text(3.8, 0.04, paste("p=", as.character(round(mean(-obs.diff < diff.permut),3)), sep=""), cex=1.2, adj = 0, font = 2, col=colarea)
text(-4.7, 0.04, paste("one-sided\np=", as.character(round(mean(obs.diff > diff.permut),3)), sep=""), cex=1.2, adj = 0, font = 2, col=colarea)

ptestlower <- quantile(probs=0.025, diff.permut) - obs.diff
ptestupper <- quantile(probs=0.975, diff.permut) - obs.diff

ptestCI <- c(ptestlower, ptestupper)
```

#### Summary of sensitivity analysis {-}

```{r}
library(knitr)
library(kableExtra)

# Create a data frame with the results
results <- data.frame(
  Method = c("t-test",
             "Welch's test",
             "Behrens' test",
             "Mann-Whitney test",
             "Permutation test"),
  `p-value` = c(biostatUZH::formatPval(tTest2$p.value),
                 biostatUZH::formatPval(WelchTest2$p.value),
                 biostatUZH::formatPval(BehrensTest2$p.value),
                 biostatUZH::formatPval(MWTest2$p.value),
                 biostatUZH::formatPval(pPermute)),
  `95% confidence interval` = c(
    paste0(format(tTest2$conf.int[1], digits=2, nsmall=2), " to ", 
           format(tTest2$conf.int[2], digits=2, nsmall=2)),
    paste0(format(WelchTest2$conf.int[1], digits=2, nsmall=2), " to ",
           format(WelchTest2$conf.int[2], digits=2, nsmall=2)),
    paste0(format(BehrensTest2$conf.int[1], digits=2, nsmall=2), " to ",
           format(BehrensTest2$conf.int[2], digits=2, nsmall=2)),
    paste0(round(MWTest2$conf.int[1], digits=2), ".00 to ",
           format(MWTest2$conf.int[2], digits=2, nsmall=2)),
    paste0(format(ptestCI[1], digits=2, nsmall=2), " to ",
           format(ptestCI[2], digits=2, nsmall=2))
  ), 
  check.names = FALSE
)

colnames(results) <- gsub("^X", "", colnames(results))  # Remove 'X' from column names if needed
# Create a kable
kable(results, 
      caption = "Sensitivity analysis",
      align = "lcc", label = NA) %>%
  kable_styling("striped", full_width = F)
```


The question now is: which test to chose? The statistical 
analysis should be pre-specified in the analysis plan, and 
the other results reported as sensitivity analyses.

#### Comparison of effect estimates {-}


Let us know define some notation. The outcome means

- at Baseline in both groups is $\mu_B$,
- at Follow-up in the control group is $\mu$, and
- at Follow-up in the treatment group is $\mu + \Delta$.

The mean difference $\Delta$ is of primary interest. We assume a common
variance $\sigma^2$ of all measurements, and $n$ observations in each group.
The correlation between
baseline and follow-up measurements is defined as $\rho$.
The estimated difference of mean follow-up measurements is denoted
by $\widehat\Delta_1$ and the estimated difference of mean change
scores by $\widehat\Delta_2$. Both estimates are unbiased (assuming baseline balance).

The variance of these estimates is
$\Var(\widehat\Delta_1) = 2\sigma^2/n$ and
$\Var(\widehat\Delta_2) = 4\sigma^2(1 - \rho)/n$, respectively.
The estimate $\widehat\Delta_2$ will thus have smaller variance than
$\widehat\Delta_1$ for $\rho > 1/2$,
so it will produce narrower confidence intervals and
more powerful tests.
In the Didgeridoo study, the estimated correlation $\hat \rho = 0.72$.

### Analysis of covariance



*Analysis of covariance* (ANCOVA) is an extension of the change score analysis:

```{r, echo = TRUE}
model3 <- lm(f.up ~ treatment + baseline)
knitr::kable(tableRegression(model3, intercept = FALSE, 
                             latex = FALSE, xtable = FALSE))
```

Now the coefficient of `baseline` is estimated from the data.

The regression diagnostics indicate a good model fit: 
```{r, echo = TRUE}
par(mfrow=c(1,2), pty="s", las=1)
plot(model3, which=1, pch=19, col=treatment, add.smooth=FALSE)
plot(model3, which=2, pch=19, col=treatment)
```


Let us denote the coefficient of the baseline variable as $\beta$.
The ANCOVA model reduces to the analysis of follow-up for $\beta = 0$, 
and to the analysis of change scores for $\beta = 1$.
The ANCOVA model estimates $\beta$ and the mean difference $\Delta$ jointly 
with *multiple regression*. The estimate $\hat\beta$ is usually close to the 
correlation $\rho$.

:::{.example #didgeridoo name="continued"}
Comparison of the three different analysis methods in the Didgeridoo study:
:::

```{r echo=T, results="asis"}
# Follow-up analysis
model1 <- lm(f.up ~ treatment)
knitr::kable(tableRegression(model1, intercept=FALSE, 
                             latex = FALSE, xtable = FALSE))

# Change score analysis
model2 <- lm(f.up ~ treatment + offset(baseline))
knitr::kable(tableRegression(model2, intercept=FALSE, 
                             latex = FALSE, xtable = FALSE))

# ANCOVA
model3 <- lm(f.up ~ treatment + baseline)
knitr::kable(tableRegression(model3, intercept = FALSE, 
                             latex = FALSE, xtable = FALSE))
```

<!-- With ${\sigma_B^2}$ and ${\sigma_F^2}$ being the  -->
<!-- variances of baseline and follow-up, it holds that -->
<!-- \[ -->
<!--   \beta = \rho \, \frac{\sigma_F}{\sigma_B}, -->
<!-- \] -->
<!-- which simplifies to $\rho$ if the variance does not change from baseline to follow-up, \ie if $\sigma_B^2 = \sigma_F^2$. -->




#### Conditioning on baseline values {-}

<!-- Let $\bar F_T$ and $\bar F_C$ be the mean follow-up values  -->
<!-- and $\bar B_T$ and $\bar B_C$ the mean baseline values in the treatment  -->
<!-- and control group, respectively. Then,  -->

<!-- \begin{eqnarray*}     -->
<!-- \widehat \Delta_1 & = & \bar F_T - \bar F_C, \\ -->
<!-- \widehat \Delta_2 & = & (\bar F_T - \bar B_T) - (\bar F_C - \bar B_C). -->
<!-- \end{eqnarray*} -->

Let $\bar {b}_T$ and $\bar {b}_C$ denote the *observed* mean baseline 
values in the current trial. 
The expectation of $\widehat\Delta_1$ and $\widehat\Delta_2$ given $\bar b_T$ and $\bar b_C$,
are 

\begin{eqnarray*}    
    \E(\widehat \Delta_1 \given \bar {b}_T, \bar {b}_C) & = & \Delta + \underbrace{\rho \cdot (\bar b_T - \bar b_C)}_{\color{red}{bias}} \\
    \E(\widehat \Delta_2 \given \bar {b}_T, \bar {b}_C) & = & \Delta + \underbrace{(\rho - 1) \cdot (\bar b_T - \bar b_C)}_{\color{red}{bias}} \\
  \end{eqnarray*}

Hence both $\widehat\Delta_1$ and $\widehat\Delta_2$ given $\bar b_T$ and $\bar b_C$
are biased if there is correlation $\rho > 0$ between baseline and follow-up measurements
and there is baseline imbalance ($\bar b_T \neq \bar b_C$).

In the Didgeridoo study there is some baseline imbalance: 
$\bar {b}_T=`r round(mean(baseline[group==1]), 1)`$, 
$\bar {b}_C=`r round(mean(baseline[group==0]), 1)`$. 

In contrast, the ANCOVA estimate $\widehat \Delta_3$ is an unbiased estimate of the mean 
difference $\Delta$ with variance

$$
\Var(\widehat \Delta_3) = 2 \sigma^2(1-\rho^2)/n,
$$
which is always smaller than the variances of 
$\widehat \Delta_1$ and $\widehat \Delta_2$. 
This means that the treatment effect estimate has a smaller standard error. 
As a result, the required sample size for ANCOVA reduces by the factor 
$\rho^2$ compared to the standard comparison of two groups 
without baseline adjustments. 



<!-- \begin{equation*} -->
<!-- \widehat \Delta_3 =    \bar F_T - \bar F_C - \rho \cdot (\bar b_T - \bar b_C) -->
<!-- \end{equation*} -->

<!-- on the other hand is an unbiased estimate of the mean difference $\Delta$ (see proof in the exercises). -->

The variances of the effect estimates in the three models can be compared by the corresponding variance factors:


$$
\Var(\widehat \Delta) = \color{red}{\mbox{variance factor}} \cdot \sigma^2 /n 
$$

___________________________________________________________


$$
\begin{aligned}
  \Var(\widehat \Delta_1) &= \color{red}{2} \cdot \sigma^2 /n \\
  \Var(\widehat \Delta_2) &= \color{red}{4 (1-\rho)} \cdot \sigma^2 /n \\
  \Var(\widehat \Delta_3) &= \color{red}{2 (1-\rho^2)} \cdot \sigma^2/n
\end{aligned}
$$


Figure \@ref(fig:varfactors) compares the variance factors of the three
models for varying correlations $\rho$.

```{r varfactors, fig.cap = "Comparison of variance factors", echo = FALSE}
n <- 1
sigma <- 1
rho <- seq(0, 1, 0.01)
var1 <- 2*sigma^2/n
var2 <- 4*sigma^2*(1-rho)/n
var3 <- 2*sigma^2*(1-rho^2)/n
par(las=1)
matplot(rho, cbind(var1, var2, var3), xlab="correlation", ylab="variance factor", type="l", lwd=2, lty=1)
legend("topright", legend=c("Follow-up analysis", "Change score analysis", "ANCOVA"), lty=1, col=1:3, lwd=2)
```

#### Least-squares means {-}

We have seen in the previous section that the ANCOVA estimate is
different from the difference of the raw mean change scores. This may
cause confusion tables reporting results from RCTs, if both raw means
of change are reported together with the ANOVA estimate of the
difference.  An alternative is to report adjusted least-squares (LS)
means via fitted values in both groups, which are compatible with the
ANCOVA estimate. Computation is illustrated with the `lsmeans` package
```{r lsmeans, fig.cap = "Least-squares means", echo = TRUE, warnings = FALSE}

library(lsmeans, quietly=TRUE)
## raw means
print(rawMeans <- ref.grid(model2))

## adjusted LS means
print(adjMeans <- ref.grid(model3, "baseline"))
```

```{r lsmeans2, echo = FALSE}
adjSummary <- summary(adjMeans)
```

The raw means `rawMeans` are simply the means of the follow-up measurements in both groups, 
whereas the adjusted least-squares means `adjMeans` are adjusted for the effect of `baseline`.
Note that `r format(adjSummary$baseline[1],digits=1, nsmall=1)`, the mean baseline value in the dataset, 
is the assumed mean baseline value in both groups.  
The ANCOVA estimate can now be calculated as the difference of the least-squares means, denoted as 
`predicted`: 
$`r round(adjSummary$prediction[2],2)` - `r format(adjSummary$prediction[1],digits=2, nsmall=2)` =
  `r format(adjSummary$prediction[2] - adjSummary$prediction[1],digits=2, nsmall=2)`$.

#### Adjusting for other variables {-}

ANCOVA allows a wide range of variables measured at baseline to be used to
adjust the mean difference. The safest approach to selecting these variables 
is to decide this *before* the trial starts (in the study protocol). Prognostic
variables used to stratify the allocation should *always* be included as covariates.

:::{.example #didgeridoo name="continued"}
In the Didgeridoo study, the mean difference has been adjusted for 
severity of the disease (`base.apnoea`) and for weight change during 
the study period (`weight.change`).
:::


```{r echo=F}
weight.change <- dweight
base.apnoea <- ah1
```

```{r echo=T, results="asis"}
model4 <- lm(f.up ~ treatment + baseline + weight.change + base.apnoea)
knitr::kable(tableRegression(model4, intercept = FALSE, latex = FALSE, xtable = FALSE))
``` 


## Additional references

Relevant references are Chapter 10 "Comparing the Means of Small Samples" 
and Chapter 15 "Multifactorial Methods" in @bland as well as Chapter 6
"Analysis of Results" in @matthews. 
Analysing controlled trials with baseline and follow up measurements
is discussed in the Statistics Note from @SN_baselineFU. Studies where the 
methods from this chapter are used in practice are for example @ravaud, @porto,
@james.

