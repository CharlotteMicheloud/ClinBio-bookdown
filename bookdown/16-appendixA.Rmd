# (APPENDIX) Appendix  {-}

#  Confidence intervals {#confint}


## Concepts

### Standard error
When we estimate some unknown parameter $\theta$, for example the mean of 
a distribution, the point estimate $\widehat \theta$ is usually imprecise. 
Repeating the same experiment over and over would give us a range of estimates. 
The standard error $\SE(\widehat \theta)$ of $\widehat \theta$ is an estimate
of the standard deviation of $\widehat \theta$ in repeated random samples. 
It is very important to understand that the terms
"standard error" and "standard deviation" are different concepts [@SN_SDvsSE]. 

### Confidence interval
The point estimate and the standard error can be used to compute a 
$\gamma\cdot 100$ \%
confidence interval (CI) for the unknown parameter $\theta$. In
practice, we choose a large confidence level $\gamma$ (between $0$ and
$1$), usually $\gamma = 0.95$. A CI is an interval estimate. Its width describes the
precision of the point estimate $\widehat \theta$.

A CI contains a range of plausible values of $\theta$. 
The exact frequentist interpretation of a CI is: 
For repeated random samples from a distribution with unknown parameter $\theta$, a $\gamma\cdot 100$ \% CI will cover $\theta$ in $\gamma\cdot 100$ \% of all cases.


## Methods

### Normal approximation
The most common construction of a CI for a parameter $\theta$ is a Wald CI, which is symmetric around the point estimate $\widehat\theta$:

\begin{equation}
(\#eq:wald)
\widehat\theta - z_\gamma\cdot \SE(\widehat\theta)\;\;\; \text{to}\;\;\; \widehat\theta + z_\gamma\cdot \SE(\widehat\theta)
\end{equation}

This construction relies on the normal approximation of the estimator in case of a large sample size. Any normal random variable $X\sim \Nor(\mu, \sigma^2)$ can be transformed into a standard normal random variable by $(X-\mu)/\sigma$. Since a standard normal random variable $Z$ is symmetric around $0$, there is a value $z_\gamma$ such that with a probability of $95$\%, $Z$ takes a value between $-z_\gamma$ and $z_\gamma$. This value $z_\gamma$ is the $(1+\gamma)/2$-quantile of the standard normal distribution. For $\gamma = 0.95$, we have $z_\gamma \approx 1.96$.

For small sample size $n$, an approximate $t$-distribution with $n-1$ degrees of freedom is more appropriate than the normal approximation. Then, a symmetric CI as in 
\@ref(eq:wald) but with $t_\gamma$ instead of $z_\gamma$ is used ($t$-test):

\begin{equation}
(\#eq:t)
\widehat\theta - t_\gamma\cdot \SE(\widehat\theta)\;\;\; \text{to}\;\;\; \widehat\theta + t_\gamma\cdot \SE(\widehat\theta)
\end{equation}


### Delta method
Suppose we know the standard error of an estimator $\widehat{\theta}$. Then, a standard error of the transformed estimator $h(\widehat{\theta})$ is given by

\begin{equation}
(\#eq:delta)
\SE\bigr(h(\widehat{\theta})\bigr) = \SE(\widehat{\theta}) \cdot \Bigr| \dfrac{dh(\widehat{\theta})}{d\theta} \Bigr|
\end{equation}

if $\widehat{\theta}$ is a consistent estimator of $\theta$ and 
$\dfrac{dh(\theta)}{d\theta}\neq 0$. This is called the *delta method* [@held, Section.Section 3.2.4, p.63].


### Difference{#appCI:difference}
Suppose we know the standard errors of two estimators $\widehat{\theta}_1, \widehat{\theta}_2$ from independent groups. Since $\Var(X-Y) = \Var(X) + \Var(Y)$ for two independent random variables $X$ and $Y$, the standard error of the difference $\Delta = \widehat{\theta}_1 - \widehat{\theta}_2$ can be calculated as

\begin{equation}
(\#eq:diff1)
\SE(\Delta) = \sqrt{\SE(\widehat{\theta}_1)^2 + \SE(\widehat{\theta}_2)^2}.
\end{equation}

This standard error can be used to compute a symmetric CI for the difference 
(in an unpaired scenario) as in \@ref(eq:wald) or \@ref(eq:t). 

#### Square-and-add method
Suppose that, for two independent groups, we know the lower and upper limits 
$l_1$ and $u_1$ of a CI for $\theta_1$ and $l_2$ and $u_2$ of a CI for 
$\theta_2$. The square-and-add method computes a CI for the difference 
$\widehat{\theta}_1 - \widehat{\theta}_2$ using these limits as follows:

\begin{equation}
(\#eq:diff2)
\Delta - \sqrt{(\widehat{\theta}_1 - l_1)^2 + (u_2 - \widehat{\theta}_2)^2}\;\;\; \text{to}\;\;\; \Delta + \sqrt{(\widehat{\theta}_2 - l_2)^2 + (u_1 - \widehat{\theta}_1)^2}
\end{equation}

When the limits of Wald CIs for $\theta_1$ and $\theta_2$ are used, then the square-and-add method results in the Wald CI for the difference. This is how this method was originally discovered. However, it is recommended to use the limits of Wilson CIs instead to improve the properties of the CI (also called Newcombe's method described in 
@newcombe1998 or @altman (Section 6, p.49).
This CI preserves the property of the Wilson CIs to be 
boundary-respecting (no overshoot).

A more complicated variant of the square-and-add method (with Wilson CIs) 
can also be used for the paired case, where both groups consist of the same 
patients (see @newcombe1998b or @altman (Section 6, p.52)).


### Substitution method
Sometimes it is preferable to compute a CI for a transformation of the 
unknown parameter $\theta$. For example, it is usually easier to look at 
log-transformed ratios such as log(RR) since the logarithm transforms a ratio 
into a difference. It is a useful trick to compute CIs on the log-scale with 
the log-transformed point estimate and then back-transform by exponentiating 
the limits of the CI. Note that when we exponentiate the limits of a symmetric 
CI, the obtained CI is no longer symmetric around the point estimate $\widehat{\theta}$.

Suppose we have calculated a $95$\% Wald CI for a log-transformed parameter $\log({\theta})$. Back-transformation to the original scale gives the $95$\% CI

\begin{equation}
(\#eq:ef)
\widehat{\theta}/\text{EF}_{.95}\; \;\; \text{to}\;\;\; \widehat{\theta}\cdot \text{EF}_{.95}
\end{equation}

for $\theta$, where

$$\text{EF}_{.95} = 
\exp\Bigr(z_\gamma \cdot \SE\bigr(\log(\widehat{\theta})\bigr)\Bigr)$$

denotes the $95$\% error factor.

The substitution method can also be useful with other transformations such as
the logit transformation or Fisher's $z$-transformation. While transformations
of the parameter to estimate also transform the CI, the $P$-value is not
affected by transformations but the reference value from the corresponding 
hypothesis test is transformed too.

### Bootstrap
Remember that a CI relies on the concept of repeated sampling. We can also 
construct a CI by simulation. This is called a bootstrap CI. A bootstrap CI
is especially helpful if the sample size is small or if we cannot assume an
underlying normal distribution. Then, we draw a large number of samples of 
the same size with replacement from the data. For all these samples, 
we compute the point estimate, for example a mean. The simplest $95$\%
bootstrap CI is given by computing the $2.5$\% and $97.5$\% quantile 
of the estimators. There are better bootstrap methods reducing bias 
but they will not be discussed here.

Bootstrap CIs have by construction a good empirical coverage and do not 
overshoot. No overshoot means that the limits of the CI lie in the range 
of values that the unknown parameter can take. For example, the limits of 
a CI for a proportion should be between $0$ and $1$. Bootstrap CIs are
usually not symmetric around the point estimate.


## Continuous outcome

In the following, we discuss commonly used CIs for continuous outcomes.

### One group

:::{.example #pima} 
The data `PimaIndiansDiabetes2`from the package `mlbench`
contains diastolic blood pressure levels
from $n=252$ diabetic women and $m=481$ non-diabetic women (missing values 
excluded). We are interested in mean blood pressure $\mu_D$ in the diabetic 
group. Denote the observations in this group by $x_1,\ldots, x_n$. Suppose
$s_D^2$ is the sample variance of $x_1,\ldots, x_n$. Then:


-  $\widehat{\mu}_D = \bar{x}$
-  $\SE(\widehat{\mu}_D) = \dfrac{s_D}{\sqrt{n}}$


We build a Wald CI for $\mu_D$ as in Equation \@ref(eq:wald):

$$\widehat{\mu}_D - z_\gamma\cdot \SE(\widehat{\mu}_D)\;\;\; \text{to}\;\;\; \widehat{\mu}_D + z_\gamma\cdot \SE(\widehat{\mu}_D)$$


The Wald CI can be computed in R:

```{r echo=TRUE, warning=FALSE}
## Data
library(mlbench)
data("PimaIndiansDiabetes2")

## Exclude missing values
ind <- which(!is.na(PimaIndiansDiabetes2$pressure))
pima <- PimaIndiansDiabetes2[ind, ]

summary(pima[, c("pressure", "diabetes")])

## Blood pressure levels for the diabetic group
ind <- which(pima$diabetes == "pos")
bpDiabetic <- pima$pressure[ind]

## Wald CI
n <- length(bpDiabetic)
mu <- mean(bpDiabetic)
se <- sd(bpDiabetic) / sqrt(n)
z <- qnorm((1+0.95)/2)
mu + (z * c(-se, +se))
```

\noindent
A symmetric CI based on the $t$-distribution can be conveniently computed in R
using a one sample $t$-test which also reports a CI.

```{r echo=TRUE}
t.test(bpDiabetic, conf.level = 0.95)
```

Wald and $t$-test CIs are very similar due to the large sample size $n=252$.
:::

### Two unpaired groups with equal variance

:::{.example}
We consider the same example as before (Example \@ref(exm:pima)).
Denote the observations in the non-diabetic group by $y_1,\ldots, y_m$, the mean by $\mu_{\bar{D}}$ and the sample variance of $y_1,\ldots, y_m$ by $s_{\bar{D}}^2$. This group is independent from the diabetic group (unpaired). We are interested in the mean difference $\Delta=\mu_D-\mu_{\bar{D}}$. If the two groups have equal variances (hence equal standard deviations), then:

-  $\widehat{\Delta} = \bar{x}-\bar{y}$
-  $\SE(\widehat{\Delta}) = s \sqrt{\dfrac{1}{m} + \dfrac{1}{n}}$, where $s^2=\dfrac{(m-1)\cdot s_{\bar{D}}^2 + (n-1)\cdot s_D^2}{m+n-2}$



We build a symmetric CI for $\Delta$ using the $t$-distribution with $m+n-2$ degrees of freedom:

$$\widehat{\Delta} - t_\gamma\cdot \SE(\widehat{\Delta})\;\;\; \text{to}\;\;\; \widehat{\Delta} + t_\gamma\cdot \SE(\widehat{\Delta})$$


In`R`,such a CI is reported in a two sample $t$-test where we need to specify the argument `var.equal = TRUE` (the default value is `FALSE`):

```{r echo=TRUE}
## Check ordering of groups
levels(pima$diabetes)
pima$diabetes <- factor(pima$diabetes, levels = c("pos", "neg"))

levels(pima$diabetes)

t.test(pressure ~ diabetes, data = pima, 
       conf.level = 0.95, var.equal = TRUE)

## Mean difference
res <- t.test(pressure ~ diabetes, data = pima, 
              conf.level = 0.95, var.equal = TRUE)
mean(res$conf.int)
```
:::

### Two unpaired groups with unequal variance

If the variances in the two groups are unequal, the standard error of the difference $\Delta$ is:


-  $\SE(\widehat{\Delta}) = \sqrt{\dfrac{s_{\bar{D}}^2}{m} + \dfrac{s_D^2}{n}}$



A Welch-Satterthwaite test using the $t$-distribution with adjusted degrees of freedom should be used.

:::{.example}
In`R`,again in Example \@ref(exm:pima), we can use `t.test` with the default argument `var.equal = FALSE` to compute the CI:

```{r echo=TRUE}
t.test(pressure ~ diabetes, data = pima,
       conf.level = 0.95, var.equal = FALSE)
```
:::

### Two paired groups

In a paired design, we look at the differences for each patient.
Then, we construct a CI as in the case of one group. 
See Example \@ref(exm:sleep).


## Binary outcome

Now, we discuss commonly used CIs for binary outcomes.
Examples in this section come from @altman (Section 6).

### One group

:::{.example}
Of $n=29$ female prisoners who did not inject drugs, $x=1$ was found to be positive on testing for HIV on discharge. We are interested in the probability $\pi$ with which such cases happen. Then:

-  $\widehat{\pi} = \dfrac{x}{n}$
-  $\SE(\widehat{\pi}) = \sqrt{\dfrac{\widehat{\pi}(1-\widehat{\pi})}{n}}$



We can build a Wald CI for $\pi$ as in \@ref(eq:wald). However, for proportions there is a better CI called Wilson CI. Especially if $n$ is small or if $\widehat{\pi}$ is very small (close to $0$) or very large (close to $1$), the Wilson CI has better properties: better empirical coverage and no overshoot. It is of the form:

\begin{equation}
(\#eq:wilson)
\dfrac{x + z_\gamma^2/2}{n + z_\gamma^2} \pm \dfrac{z_\gamma \sqrt{n}}{n + z_\gamma^2} \sqrt{\widehat{\pi}(1-\widehat{\pi}) + \dfrac{z_\gamma^2}{4n}}
\end{equation}


This formula is complicated. In practice, we use the package `biostatUZH` 
where both Wald and Wilson CIs for proportions are implemented.

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(biostatUZH)

## Data
x <- 1
n <- 29

wald(x, n, conf.level = 0.95)
wilson(x, n, conf.level = 0.95)
```


Note that when the Wald CI overshoots, the function `wald ()` returns a CI that is truncated such that the limits lie between $0$ and $1$.

Also the function `prop.test()` computes the Wilson CI: 
```{r echo=TRUE}
prop.test(x, n, conf.level = 0.95, correct = FALSE)
``` 
:::

### Two unpaired groups

:::{.example}
A collaborative clinical trial assessed the value of extracorporeal 
membrane oxygenation for term neonates with severe respiratory failure.
$63$ out of $93$ patients randomised to active treatment survived to one
year compared with $38$ out of $92$ infants who received conventional
management. Let $\pi_1$ and $\pi_2$ be the probabilities to survive up
to one year in the two groups.
:::
The two groups can be compared using different effect measures such as the 
absolute risk reduction (or risk difference or probability difference), 
the relative risk reduction (or risk ratio or relative risk) and the (sample) 
odds ratio. These effect measures are computed from the following $2\times 2$ table:

```{r echo=FALSE, results='asis'}

m <- as.data.frame(matrix(c("$a=x_1=63$", "$c=x_2=38$", 
                            "$b=n_1-x_1=30$", "$d=n_2-x_2=54$", 
                            "$n_1=93$", "$n_2=92$"), 
                          nrow = 2))
rownames(m) <- c("Treatment", "Control")
colnames(m) <- c("Survived", "Not survived", "Total")
library(knitr)
kable(m, format = "latex", escape = FALSE, align = c('l', 'r', 'r', 'r'))
```


For the absolute risk reduction $\ARR = \pi_1 - \pi_2$, we have:


-  $\widehat{\ARR} = \dfrac{x_1}{n_1} - \dfrac{x_2}{n_2}$
-  $\SE(\widehat{\ARR}) = \sqrt{\dfrac{\widehat{\pi}_1(1-\widehat{\pi}_1)}{n_1} + \dfrac{\widehat{\pi}_2(1-\widehat{\pi}_2)}{n_2}}$



The standard error is computed using equation \@ref(eq:diff1) for the difference.

Wald and Wilson CIs can be computed in`R`using the package 
`biostatUZH`. A Wald CI uses the above standard error.
A Wilson CI uses the square-and-add method with Wilson CIs 
for $\pi_1$ and $\pi_2$. Also here, Wilson CIs have better properties.

```{r echo=TRUE, message=FALSE, warning=FALSE}
## Data
tabRespiratory <- matrix(c(63, 38, 30, 54), nrow = 2)
colnames(tabRespiratory) <- c("Survived", "Not survived")
rownames(tabRespiratory) <- c("Treatment", "Control")

print(tabRespiratory)

confIntRiskDiff(x = tabRespiratory[, 1], 
                n = rowSums(tabRespiratory), 
                conf.level = 0.95)
```


For the risk ratio $\RR=\pi_1/\pi_2$, we have:


-  $\widehat{\RR} = \dfrac{x_1/n_1}{x_2/n_2}$
-  $\log(\widehat{\RR}) = \log\Bigr(\dfrac{x_1/n_1}{x_2/n_2}\Bigr)$
-  $\SE\bigr(\log(\widehat{\RR})\bigr) = \sqrt{\dfrac{1}{x_1} - \dfrac{1}{n_1} + \dfrac{1}{x_2} - \dfrac{1}{n_2}}$



The standard error is computed using equation \@ref(eq:diff1) for the 
difference of two log proportions. The standard error of a log proportion 
can be computed using the standard error of a proportion and the delta method:

-  $\SE\bigr(\log(\widehat{\pi}_1)\bigr) = \sqrt{\dfrac{1}{x_1} - \dfrac{1}{n_1}}$


We use the substitution method to compute a CI:

CI for the $\log(\widehat{\RR})$:
$$\log(\widehat{\RR}) - z_\gamma\cdot \SE\bigr(\log(\widehat{\RR})\bigr)\;\;\; \text{to}\;\;\; \log(\widehat{\RR}) + z_\gamma\cdot \SE\bigr(\log(\widehat{\RR})\bigr)$$

CI for the $\widehat{\RR}$:
$$\exp\Bigr(\log(\widehat{\RR}) - z_\gamma\cdot \SE\bigr(\log(\widehat{\RR})\bigr)\Bigr)\;\;\; \text{to}\;\;\; \exp\Bigr(\log(\widehat{\RR}) + z_\gamma\cdot \SE\bigr(\log(\widehat{\RR})\bigr)\Bigr)$$

which is the same as

$$\widehat{\RR}/\text{EF}_{.95}\;\;\; \text{to}\;\;\; \widehat{\RR}\cdot \text{EF}_{.95}$$

For the odds ratio $\OR=\dfrac{\pi_1/(1-\pi_1)}{\pi_2/(1-\pi_2)}$, we have:


-  $\widehat{\OR} = \dfrac{a\cdot d}{b\cdot c}$
-  $\log(\widehat{\OR}) = \log\Bigr(\dfrac{a\cdot d}{b\cdot c}\Bigr)$
-  $\SE\bigr(\log(\widehat{\OR})\bigr) = \sqrt{\dfrac{1}{a} + \dfrac{1}{b} + \dfrac{1}{c} + \dfrac{1}{d}}$


The standard error is computed using equation \@ref(eq:diff1) for the difference of two log odds. The standard error of the log odds can be computed from the standard error of a proportion and the delta method:


-  $\SE\bigr(\log(\widehat{\pi_1}(1-\widehat{\pi_1}))\bigr) = \sqrt{\dfrac{1}{a} + \dfrac{1}{b}}$


We use the substitution method to compute a CI:

CI for the $\log(\widehat{\OR})$:
$$\log(\widehat{\OR}) - z_\gamma\cdot \SE\bigr(\log(\widehat{\OR})\bigr)\;\;\; \text{to}\;\;\; \log(\widehat{\OR}) + z_\gamma\cdot \SE\bigr(\log(\widehat{\OR})\bigr)$$

CI for the $\widehat{\OR}$:
$$\exp\Bigr(\log(\widehat{\OR}) - z_\gamma\cdot \SE\bigr(\log(\widehat{\OR})\bigr)\Bigr)\;\;\; \text{to}\;\;\; \exp\Bigr(\log(\widehat{\OR}) + z_\gamma\cdot \SE\bigr(\log(\widehat{\OR})\bigr)\Bigr)$$

which is the same as

$$\widehat{\OR}/\text{EF}_{.95}\;\;\; \text{to}\;\;\; \widehat{\OR}\cdot \text{EF}_{.95}$$


In`R`,Wald CIs for OR and RR and a Wilson CI for ARR can be computed using the function `twoby2()` from package `Epi`.

```{r echo=TRUE}
library(Epi)

twoby2(tabRespiratory, alpha = 0.05)
```

### Two paired groups

:::{.example #ischemia}
In a reliability exercise carried out as part of the Multicenter Study of Silent Myocardial Ischemia, $41$ patients were randomly selected from those who had undergone a thallium-$201$ stress test. The $41$ sets of images were classified as normal or ischemic by the core thallium laboratory and, independently, by clinical investigators from different centers who had participated in training sessions to support standardization.

This is a paired design. The results for one of the participating clinical investigators compared to the core laboratory are presented in a contingency table showing the frequency distribution of these two variables:

```{r echo=FALSE, results='asis'}
library(knitr)
library(kableExtra)
# Create the data frame
m <- as.data.frame(matrix(c("$14$", "$0$", "$14$", 
                            "$5$", "$22$", "$27$", 
                            "$19$", "$22$", "$n=41$"), nrow = 3))
rownames(m) <- c("Ischemic", "Normal", "Total")
colnames(m) <- c("Ischemic", "Normal", "Total")

# Add custom header rows with LaTeX formatting
header <- "\\multicolumn{4}{c}{Core laboratory} \\\\ \\hline
Clinical investigator & Ischemic & Normal & Total \\\\"

# Create the table with kable
knitr::kable(m, format = "latex", booktabs = TRUE, escape = FALSE, align = "r",
             col.names = NULL) %>%
  kableExtra::add_header_above(header = c(" " = 1, "Core laboratory" = 3)) %>%
  kableExtra::row_spec(0, bold = TRUE, align = "c")
```

The square-and-add method for paired data can be used to compute a Newcombe CI. This method is implemented in the package `biostatUZH` where the CI is computed from the contingency table for "Ischemic vs. Normal":

```{r echo=TRUE}
## Data
tabIschemia <- matrix(c(14, 0, 5, 22), nrow = 2)
colnames(tabIschemia) <- c("Lab ischemic", "Lab normal")
rownames(tabIschemia) <- c("Clin ischemic", "Clin normal")

print(tabIschemia)

confIntPairedProportion(tabIschemia, conf.level = 0.95)
```
:::



## Confidence interval for a sample variance and standard deviation {#CIvar}

Let $S^2 = \frac{1}{n - 1} \sum_{i = 1}^{n}(X_i - \bar X)^2$
denote the sample variance. Standard statistical theory shows 
[@held, Section 3.2] that, under normality and independence of 
$X_i \sim \Nor(\mu, \sigma^2)$, $i = 1, \ldots n$, 

$$
(\#eq:chisq)
\frac{n - 1}{\sigma^2} S^2 \sim \chi^2(n - 1)
$$
holds, 
so $\E(S^2) = \sigma^2$, i.e. $S^2$ is unbiaised for $\sigma^2$.

Let $\chi_\alpha^2(k)$ denote the $\alpha$-quantile of the $\chi^2(k)$
distribution. It follows that 

$$
(\#eq:CIsigma2)
\left[\frac{(n - 1)S^2}{\chi^2_{(1 + \gamma)/2} (n - 1) }, \frac{(n - 1)S^2}{\chi^2_{(1 - \gamma)/2} (n - 1) }\right]
$$

is a $\gamma \times 100\%$ CI for $\sigma^2$. 
Likewise, the 
square root of \@ref(eq:CIsigma2) gives a $\gamma \times 100\%$ CI for the 
standard deviation $\sigma$.


We may also construct Wald CI based on the standard error of $S^2$ 
and $S$, respectively. 
From \@ref(eq:chisq) it follows that 

$$
\frac{(n - 1)^2}{\sigma^4} \Var(S^2) = 2(n - 1),
$$
so $\Var(S^2) = \frac{2}{n - 1} \sigma^4$
and therefore $\SE(S^2) = \sqrt{\frac{2}{n - 1}}S^2$. 

Application of the Delta method shows that 

$$
\SE(S) = \SE(S^2) \cdot \frac{1}{2S} = \frac{S}{\sqrt{2(n - 1)}} \approx
\frac{S}{\sqrt{2n}}.
$$
These results allow to construct standard Wald CI for the 
within-subject standard deviation $s_w$ (Section \@ref(measErr))
and the limits of agreements in a Bland-Altman plot (Section \@ref(CIloa)),
for example. 

# Statistical tests and $P$-values {#Pvalues}

## Concepts

### $P$-value
The $P$-value $p$ is a value between $0$ and $1$ which can be used for
hypothesis testing or for significance testing. We test for evidence 
against a point null hypothesis H$_0$ which is given by a reference value,
usually $0$. For example, H$_0$ could be the hypothesis that there is no 
treatment effect.
The $P$-value is correctly interpreted as the probability, under the 
assumption of the null hypothesis H$_0$, of obtaining a result equal 
to or more extreme than what was actually observed.

### Hypothesis testing
The type I error rate $\alpha$ is the probability of rejecting $H_0$ 
although $H_0$ is true. Hypothesis testing wants to control the probability
for this type of error by keeping it small, usually $\alpha = 0.05$. 
In practice, the $P$-value is compared to the threshold $\alpha$ and 
rejects $H_0$ if and only if $p\leq \alpha$.

#### Relation to confidence interval
A $\gamma\cdot 100$\% confidence interval (CI) can be used to carry 
out a hypothesis test with $\alpha= 1-\gamma$ by rejecting H$_0$ if
and only if the CI does not contain the reference value. If the reference
value lies just on the boundary of the CI, then the $P$-value is $1-\gamma$.
Below, we show how to compute a $P$-value from a CI in case of a $z$-test or
a $t$-test, see also @altman1, @altman2.

Suppose we know the estimate $\widehat{\theta}$ and the lower and upper 
limits $l$ and $u$ of the $\gamma\cdot 100$\% Wald CI. Using the definition 
of the Wald CI, we can compute the standard error:


\begin{equation*}
\SE(\widehat{\theta})=\dfrac{u-l}{2z_\gamma}
\end{equation*}


From this, we can compute what was observed for the test statistic to compute
the $P$-value:

\begin{equation*}
p = 2\bigr(1-\text{pnorm}(\widehat{\theta}/\SE(\widehat{\theta}))\bigr)
\end{equation*}

The same can be done if we know the symmetric CI based on the $t$-distribution
by exchanging $z_\gamma$ and `pnorm()` with $t_\gamma$ and `pt()` using the
correct degrees of freedom for the $t$-distribution.

While the $P$-value quantifies the strength of evidence in one number,
the CI shows the effect size and the amount of uncertainty. This is different 
information, hence both $P$-value and CI should be reported.

### Significance testing 

Significance testing uses the $P$-value to quantify the strength of evidence 
against the null hypothesis. Instead of a strict threshold, the interval from 
$0$ to $1$ is divided into regions of "weak evidence", "strong evidence" etc. 
(see Figure \@ref(fig:strengthofevidence)).


## Continuous outcome

Commonly used tests for continuous outcomes with known variances are the
$z$- and $t$-test. The $z$-test should only be used if the sample size is 
large. Otherwise, the $t$-test is more appropriate.

### $z$-test {#sec:ztest}

If we know the reference distribution of the test statistic under 
the null hypothesis, it can be used to compute the $P$-value. We consider 
the estimated test statistic

$$Z=\widehat{\theta}/\SE(\widehat{\theta}),$$

where $\widehat{\theta}$ is the estimate of the unknown parameter
$\theta$ and $\SE(\widehat{\theta})$ is the standard error. Using normal 
approximation of the estimator for large sample size, 
$Z$ follows approximately a standard normal distribution ($z$-test or also Wald test).


#### Application in one group

:::{.example #pima2}
In Example \@ref(exm:pima), let the null hypothesis be $H_0$: $\mu_D = 73$.
Two-sided or one-sided $P$-values from a $z$-test can be computed as follows:

```{r warning=FALSE, message=FALSE, echo=TRUE}
## Data
library(mlbench)
data("PimaIndiansDiabetes2")

## Exclude missing values
ind <- which(!is.na(PimaIndiansDiabetes2$pressure))
pima <- PimaIndiansDiabetes2[ind, ]

## Blood pressure levels for the diabetic group
ind <- which(pima$diabetes == "pos")
diab_bp <- pima$pressure[ind]

n <- length(diab_bp)
mu <- mean(diab_bp-73)
se <- sd(diab_bp-73) / sqrt(n)

## two-sided p-value
library(biostatUZH)
printWaldCI(theta = mu, se.theta = se, conf.level = 0.95)

## one-sided p-value
test_stat <- mu/se
pnorm(q = test_stat, mean = 0, sd = 1, lower.tail = FALSE) # H_0: <73
pnorm(q = test_stat, mean = 0, sd = 1, lower.tail = TRUE)  # H_0: >73
```

Note that the test is performed by shifting the blood pressure levels by $73$ and then testing if these shifted blood pressure levels are different from $0$.
:::


### $t$-test {#sec:ttest}

The $t$-test assumes independent measurements in two groups that are normally
distributed with equal variances:

\begin{eqnarray*}
\text{Treatment: }&Y_1, \ldots, Y_{m} &\sim \Nor(\mu_T, \sigma_T^2)\\
\text{Control: }&X_1, \ldots, X_{n} &\sim \Nor(\mu_C, \sigma_C^2)
\end{eqnarray*}

The sample sizes are $m$ and $n$, respectively, and the equal variances
assumption implies that $\sigma_T^2=\sigma_C^2$ The quantity of interest is the *mean difference* $\Delta = \mu_T-\mu_C$. The null hypothesis is

$$H_0: \Delta = 0.$$

The estimate of $\Delta$ is the *difference in sample means*

$$\widehat \Delta = \bar{Y} - \bar{X}$$

with standard error

\begin{equation*}
\SE(\widehat \Delta) = s \cdot \sqrt{\frac{1}{m} + \frac{1}{n}},
\end{equation*}

where

$$s^2 = \frac{(m-1) s_T^2 +(n-1) s_C^2}{m+n-2}$$

is an estimate of the common variance $\sigma^2$. Here, $s_T^2$ and $s_C^2$ are
the estimates of the variances $\sigma_T^2$ and $\sigma_C^2$ in the two groups. The $t$-test statistic is

$$T = \frac{\widehat \Delta}{\SE(\widehat \Delta)}.$$

Assuming $H_0$ is true, the test statistic $T$ follows a 
$t$-distribution with $m+n-2$ "degrees of freedom" (df). 
In case of only one group of size n,
it is a $t$-distribution with $n-1$ degrees of freedom.



## Binary outcome

For binary outcomes, Wald CIs are often not appropriate.
However, $P$-values cannot be easily computed from CIs other than Wald or 
$t$-test CIs. In these cases, it is common to use the $\chi^2$-test, Fisher's 
exact test for small samples or McNemar's test for paired data. These tests 
generally test the null hypothesis that the events of an investigated factor 
(or the proportions) in two groups are independent.

### $\chi^2$-test

The $\chi^2$-test computes the *expected number of cases* $e_i$
in each cell $i=1,\ldots,4$ of a $2 \times 2$ table under the assumption of no
difference between the groups.


```{r apsacappendix}
# Create the data frame
apsac_data <- data.frame(
  Therapy = c("APSAC", "Heparin", "total"),
  dead = c(9, 19, 28),
  alive = c(153, 132, 285),
  total = c(162, 151, 313)
)

# Display the table using kable
kable(apsac_data, col.names = c("Therapy", "Dead", "Alive", "Total"),
      caption = "Results of the APSAC Study from Example \\@ref(exm:apsac)", 
       align = c("l", "l", "l", "l"))


```





The expected number of cases in a particular cell are defined as the product 
of the corresponding row and column sums divided by the total number of 
participants. For example, the expected number of cases in the first cell of
Table \@ref(tab:apsacappendix) are 

\begin{equation*}
\left(\frac{162}{313}\cdot \frac{28}{313}\right) \cdot 313 = 
\frac{162\cdot 28}{313} = `r round(162*28/313,2)`, 
\end{equation*}

to be compared with 9 observed cases. The part in the bracket is the 
probability to lie in this cell based on the marginal frequencies 
(in this case, in the first row and first column). Multiplying this 
probability with the total number of participants yields the expected number of 
cases in this cell. The expected frequencies for agreement by chance are 
calculated in the same way (Table \@ref(tab:expected)).

The expected number of cases $e_i$ are then compared with the observed number
of cases $y_i$ based on the test statistic
\[
  T = \sum_i \frac{(y_i - e_i)^2}{e_i}.
\]
This test statistic follows a $\chi^2$-distribution with 1 degree of
freedom under the null hypothesis of no difference between the two
groups, so a $P$-value can be easily calculated. Note that it is a two-sided
test due to the quadratic differences where it does not matter whether $y_i<e_i$ or $e_i<y_i$.

The $\chi^2$-test with *continuity correction* is based on the modified
test statistic
\[
  T = \sum_i \frac{\left((\abs{y_i - e_i} - 0.5)_+\right)^2}{e_i},
\]
here $x_+ = \max\{x, 0\}$. What we illustrated here for $2 \times 2$ tables can also be generalized to more categories.




#### Application in two paired groups

:::{.example}
In Example \@ref(exm:ischemia) with paired data, a McNemar test can be performed in 
R as follows (default is with continuity correction):

```{r echo=TRUE}
## Data
tabIschemia <- matrix(c(14, 0, 5, 22), nrow = 2)
colnames(tabIschemia) <- c("Lab ischemic", "Lab normal")
rownames(tabIschemia) <- c("Clin ischemic", "Clin normal")

print(tabIschemia)

## With continuity correction
mcnemar.test(x = tabIschemia, correct = TRUE)

## Without continuity correction
mcnemar.test(x = tabIschemia, correct = FALSE)
```
:::

### Fisher's exact test

Fisher's exact test is based on the probabilities of all possible tables 
with the observed row and column totals under the null hypothesis of 
no difference between the groups. It is a two-sided test, there are three 
different versions and it can also be generalized to more categories.



## Survival outcome
For survival outcomes, the log-rank test can be used to compare two treatment 
groups. The log-rank test gives a $P$-value based on the expected number of 
events under the null hypothesis of no difference between the two groups. 
An estimate of the hazard ratio with CI can be derived from the observed and 
expected number of events in the two groups. 




# Some proofs on ROC curves {#ROC}

<!-- {#sec:appendixROC} -->

## The ROC curve {#sec:ROC}

Assume we have a continuous (in mathematical terminology "absolutely
continuous") result $Y$ from a diagnostic test and denote the true and
false positive fraction for a given threshold $c$

\begin{eqnarray*}
\mbox{TPF}(c) & = & \Pr(Y \geq c \given D=1) \\
\mbox{FPF}(c) & = & \Pr(Y \geq c \given D=0).
\end{eqnarray*}

The test result $Y$ will have different distributions in the diseased ($D=1$) and
non-diseased ($D=0$) population and we will denote the corresponding random variable
by $Y_D$ and $Y_{\bar D}$, respectively.


Further define the ROC curve via

\begin{eqnarray*}
\mbox{ROC}(.) & = & \{(\mbox{FPF}(c), \mbox{TPF}(c)), c \in (-\infty, \infty)\},
\end{eqnarray*}

\ie the ROC curve are the points $(\mbox{FPF}(c), \mbox{TPF}(c))$ for
all possible thresholds $c$. Now define
the survivor functions in the diseased and non-diseased populations:

\begin{eqnarray*}
S_D(y) & = & \Pr(Y \geq y \given D=1) = \mbox{TPF}(y) \\
S_{\bar D}(y) & = & \Pr(Y \geq y \given D=0) = \mbox{FPF}(y).
\end{eqnarray*}

Note that the survivor functions are strictly monotone and hence invertible.

For a given threshold $c$, suppose $t=\mbox{FPF}(c)= S_{\bar D}(c)$, so
$c=S_{\bar D}^{-1}(t)$ and we obtain
\[
\mbox{ROC}(t) =  \mbox{TPF}(c) = S_D(c)  = S_D(S_{\bar D}^{-1}(t)).
\]


## AUC {#sec:AUC}

The area under the curve (AUC) is defined as
\[
\mbox{AUC} = \int_0^1 \mbox{ROC}(t)dt
\]
and we have

\begin{equation}
(\#eq:eq2)
\mbox{AUC} = \Pr(Y_D > Y_{\bar D}), 
\end{equation}

if $Y_D$ and $Y_{\bar D}$ are independent test results from the diseased
and non-diseased
populations, respectively.


To proof this result, we first note that the derivative of a survivor function $S(y)$ of
a continous test result $Y$ is
closely related to the cumulative distribution function $F(y)$ of $Y$:
\[
S(y) = \Pr(Y \geq y) = 1 - \Pr(Y < y) = 1 - \Pr(Y \leq y) =  1 - F(y)
\]
Now
\begin{eqnarray*}
\partials{F(y)}{y} = f(y)
\end{eqnarray*}
where $f(y)$ denotes the density function of $Y$. Therefore
\[
\partials{S(y)}{y} = -f(y).
\]

Now consider
\begin{eqnarray}
 (\#eq:eq1)
\mbox{AUC} &=& \int_0^1 \mbox{ROC}(t)dt \nonumber \\
&=& \int_0^1 S_D(S_{\bar D}^{-1}(t))dt \nonumber \\
&=& \int_{\infty}^{-\infty} S_D(y) \left\{ -f_{\bar D}(y) dy \right\}\nonumber  \\
&=& \int_{-\infty}^{\infty} S_D(y) f_{\bar D}(y) dy,
\end{eqnarray}
where we have applied the substitution $t=S_{\bar D}(y)$, so
\[
dt = d S_{\bar D}(y) = -f_{\bar D}(y)dy.
\]
Let $\Ind\{A\}$ denote the indicator function of an event $A$.
Now $Y_D$ and $Y_{\bar D}$ are assumed to be independent, so
\begin{eqnarray*}
\Pr(Y_D \geq Y_{\bar D}) &=& \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \Ind\{y_D \geq y_{\bar D}\} f(y_D, y_{\bar D}) dy_D dy_{\bar D} \\
&=& \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \Ind\{y_D \geq y_{\bar D}\} f_D(y_D) f_{\bar D}(y_{\bar D}) dy_D dy_{\bar D} \\
&=& \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \left[ \Ind\{y_D \geq y_{\bar D}\} f_D(y_D)dy_D \right] f_{\bar D}(y_{\bar D}) dy_{\bar D} \\
&=& \int_{-\infty}^{\infty} \Pr(Y_D \geq y_{\bar D}) f_{\bar D}(y_{\bar D}) dy_{\bar D} \\
&=& \int_{-\infty}^{\infty} S_D(y_{\bar D}) f_{\bar D}(y_{\bar D}) dy_{\bar D}
\end{eqnarray*}

where the last equation is equal to \@ref(eq:eq1) with $y_{\bar D}=y$. 
This proofs equation \@ref(eq:eq2).



# Some theory on equivalence trials {#equiv}

## Type I error in equivalence trials 

Assume we have a continuous outcome with variance $\sigma^2$ and two
treatment groups of size $n_A$ and $n_B$, respectively. Now consider
the difference $\bar d$ of the mean outcomes as treatment effect, so
$\bar d \sim \Nor(\Delta, \sigma^2 \, \lambda^2)$ where 
the true treatment difference is $\Delta$ and $\lambda =
\sqrt{1/n_A+1/n_B}$. 



Equivalence is established if the $\gamma \cdot 100$ \% confidence
interval for the treatment difference is within the pre-specified
*interval of equivalence* $(-\delta, \delta)$, i.e.
  \[
  \bar d \pm z_{(1+\gamma)/2} \, \sigma \, \lambda \subset (-\delta, \delta).
  \]
With $z = z_{(1+\gamma)/2}$ we obtain the equivalent requirement 
  \[
  \bar d \in  (-\xi, \xi) \, \mbox{ where } \xi = \delta - z_{(1+\gamma)/2} \, \sigma \, \lambda
  \]
  
  to establish equivalence.  This happens with probability
  
\begin{eqnarray}
(\#eq:eq1App)
\P(\bar d \in (-\xi, \xi) ) &=& \Phi\left(\frac{\xi - \Delta }{\lambda \, \sigma}\right)
- \Phi\left(\frac{-\xi - \Delta }{\lambda \, \sigma}\right) \nonumber \\
&=& \Phi\left(\frac{\delta - \Delta }{\lambda \, \sigma} - z_{(1+\gamma)/2}\right)
- \Phi\left(\frac{-\delta - \Delta }{\lambda \, \sigma} + z_{(1+\gamma)/2}\right).
\end{eqnarray}

Now evaluate \@ref(eq:eq1App) at $\Delta = \delta$ to obtain 
the  *Type I error rate* (more precisely an upper bound on the
Type I error rate)

\begin{eqnarray}
(\#eq:eq2App)
\alpha = \Phi(-z_{(1+\gamma)/2}) - \Phi\left(z_{(1+\gamma)/2} - \frac{2 \delta}{\lambda \, \sigma}\right).
\end{eqnarray}

Likewise, the *power* $1 - \beta$ is derived from \@ref(eq:eq1App) with
$\Delta = 0$:

\[
1 - \beta = 2 \Phi\left(\frac{\delta}{\lambda \, \sigma} - z_{(1+\gamma)/2}\right) - 1
\]
so
\[
1 - \beta/2 = \Phi\left(\frac{\delta}{\lambda \, \sigma} - z_{(1+\gamma)/2}\right).
\]

Note that $1 - \beta/2 = \Phi(z_{1-\beta/2})$, and therefore

\begin{eqnarray}
(\#eq:eq3App)
\frac{\delta}{\lambda \, \sigma} = z_{(1+\gamma)/2} + z_{1-\beta/2}.
\end{eqnarray}

With \@ref(eq:eq2App) we obtain

\begin{eqnarray}
(\#eq:eq3App)
\alpha \approx \Phi(-z_{(1+\gamma)/2}) = 1-(1+\gamma)/2 = (1-\gamma)/2
\end{eqnarray}

for relatively small $\beta$ (say  $\beta \leq 30$ \%), since the second term 
in \@ref(eq:eq2App) 

\[
\Phi\left(z_{(1+\gamma)/2} - \frac{2 \delta}{\lambda \, \sigma}\right) = \Phi(z_{(1+\gamma)/2} - 2 \, (z_{(1+\gamma)/2} + z_{1-\beta/2})) = \Phi(-z_{(1+\gamma)/2} - 2 \, z_{1-\beta/2})
\]

is then very close to zero. Note that \@ref(eq:eq3App)  differs from
superiority trials, where $\alpha = 1-\gamma$.


## Sample size calulations in equivalence trials 

Assume equal group sizes, i.e. $n=n_A=n_B$, then $\lambda=\sqrt{2/n}$.

With \@ref(eq:eq3App) we then obtain

\begin{eqnarray*}
n &=& \frac{2 \sigma^2 (z_{(1+\gamma)/2} + z_{1-\beta/2})^2}{\delta^2} \\
&=& \frac{2 \sigma^2 (z_{1-\alpha} + z_{1-\beta/2})^2}{\delta^2}
\end{eqnarray*}

as the required sample size in each group for Type I error rate $\alpha$
and power $1-\beta$.


# Expectation, Variance, Covariance and Correlation
This section describes fundamental summaries of the distribution of random variables, namely their expectation and variance. For multivariate random variables, the association between the components is summarised by their covariance or correlation. Moreover we list some useful inequalities.

## Expectation {#sec:expectation}

For a continuous random variable $X$ with density function $\p(x)$, the **expectation** or **mean value** of $X$ is the real number

\begin{equation}
\E (X) = \int x \p(x) \, dx
(\#eq:expectation)
\end{equation}

The expectation of a function $h(X)$ of $X$ is

$$
\E \{h(X)\} = \int h(x) \p(x) \, dx
(\#eq:expectation2)
$$

Note that the expectation of a random variable not necessarily exists. If the integral exists then $X$ has **finite expectation**; otherwise, we say $X$ has **infinite expectation**.

For a discrete random variable $X$ with probability mass function $\p(x)$, the integral in \@ref(eq:expectation) and \@ref(eq:expectation2) is replaced with a sum over the support of $X$.

For any real numbers $a$ and $b$:

$$
\E(a \cdot X + b) = a \cdot \E(X) + b
$$

For any two random variables $X$ and $Y$:

$$
\E(X + Y) = \E(X) + \E(Y)
$$

If $X$ and $Y$ are independent:

$$
\E(X \cdot Y) = \E(X) \cdot \E(Y)
$$

The expectation of a $p$-dimensional random variable $\boldsymbol{X} = (X_1, \dotsc, X_p)^{\top}$ is:

$$
\E(\boldsymbol{X}) = (\E(X_1), \dotsc, \E(X_p))^{\top}
$$

The expectation of a real-valued function $h(\boldsymbol{X})$ of a $p$-dimensional random variable $\boldsymbol{X} = (X_1, \dotsc, X_p)^{\top}$ is

$$
\E \{h(\boldsymbol{X})\} = \int h(\boldsymbol{X}) \p(\boldsymbol{X}) \, d\boldsymbol{x}
(\#eq:expectation3)
$$

## Variance

The **variance** of a random variable $X$ is

$$
\Var (X) = \E \{X - \E(X)\}^{2}
$$

It can also be expressed as

$$
\Var (X) = \E (X^{2}) - \E(X)^{2}
$$

and

$$
\Var (X) = \frac{1}{2}\E\left\{(X_1 - X_2)^2 \right\}
$$

where $X_1$ and $X_2$ are independent copies of $X$. The square root $\sqrt{\Var(X)}$ is called the **standard deviation**.

For real numbers $a$ and $b$:

$$
\Var(a \cdot X + b) = a^2 \cdot \Var(X)
$$

## Moments {#sec:moments}

Let $k$ be a positive integer. The **$k$-th moment** $m_k$ of a random variable $X$ is

$$
m_k = \E(X^k)
$$

The **$k$-th central moment** is

$$
c_k = \E \{(X - m_1)^k\}
$$

The expectation is the first moment, and the variance is the second central moment.

## Conditional Expectation and Variance {#sec:conditionalexpectationUndVariance}

For continuous random variables, the **conditional expectation** of $Y$ given $X = x$ is

$$
\E (Y\mid X = x) = \int y \p(y\mid x) \, dy
(\#eq:conditionalexpectation)
$$

For discrete variables, the integral becomes a sum. The **conditional variance** of $Y$ given $X = x$ is

$$
\Var (Y\mid X = x) = \E \left[ \{Y - \E (Y\mid X = x)\}^{2}\mid X = x\right]
(\#eq:conditionalVariance)
$$

If we treat $x$ as unknown, then $g(X) = \E(Y\mid X)$ and $h(X) = \Var(Y\mid X)$ become random variables themselves.

Two key results:

**Law of total expectation**:

$$
\E (Y) = \E\{\E (Y\mid X)\}
(\#eq:iterierterEwert)
$$

**Law of total variance**:

$$
\Var (Y) = \E\{\Var (Y\mid X)\} + \Var\{\E (Y\mid X)\}
(\#eq:variancezerlegungssatz)
$$

These are useful when conditional moments are known.

## Covariance {#sec:covariance}

Let $(X, Y)^{\top}$ be a bivariate random variable. The **covariance** is

$$
\Cov(X,Y) = \E\bigl[\{X-\E(X)\}\{Y-\E(Y)\}\bigr] = \E(XY)-\E(X)\E(Y)
$$

See \@ref(eq:expectation3). Note: $\Cov (X, X)= \Var (X)$ and $\Cov (X, Y)= 0$ if $X$ and $Y$ are independent.

For real $a$, $b$, $c$, $d$:

$$
\Cov(a X + b, c Y + d) = a c \Cov(X,Y)
$$

For $\boldsymbol{X} = (X_1, \dotsc, X_p)^{\top}$:

$$
\Cov(\boldsymbol{X}) = \E\left[\{\boldsymbol{X}- \E(\boldsymbol{X})\}\{\boldsymbol{X}- \E(\boldsymbol{X})\}^{\top} \right]
$$

Also:

$$
\Cov(\boldsymbol{X}) = \E(\boldsymbol{X} \boldsymbol{X}^{\top}) - \E(\boldsymbol{X}) \E(\boldsymbol{X})^{\top}
$$

For a matrix $\boldsymbol{A}$:

$$
\Cov(\boldsymbol{A} \boldsymbol{X}) = \boldsymbol{A} \Cov(\boldsymbol{X}) \boldsymbol{A}^{\top}
$$

In particular:

$$
\Var(X+Y) = \Var(X) + \Var(Y) + 2 \Cov(X,Y)
$$

If $X$ and $Y$ are independent:

$$
\Var(X+Y) = \Var(X) + \Var(Y)
$$

$$
\Var(XY) = \E(X)^2 \Var(Y) + \E(Y)^2 \Var(X) + \Var(X) \Var(Y)
$$

## Correlation {#korrCauchySchwarz}

The **correlation** of $X$ and $Y$ is

$$
\Corr(X,Y) = \frac{\Cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}}
$$

as long as variances are positive.

An important inequality:

$$
\left|\Corr(X,Y)\right| \leq 1
(\#eq:korrUngleichung)
$$

This follows from the **Cauchy--Schwarz inequality**:

$$
\E(XY)^2 \leq \E(X^2)\E(Y^2)
(\#eq:CauchySchwarz)
$$

Apply \@ref(eq:CauchySchwarz) to $X-\E(X)$ and $Y-\E(Y)$ to get \@ref(eq:korrUngleichung).

If $Y = aX + b$ with $a > 0$, then $\Corr(X, Y) = 1$; if $a < 0$, then $\Corr(X, Y) = -1$.

Let $\boldsymbol{\Sigma}$ be the covariance matrix. The **correlation matrix** $\boldsymbol{R}$ is:

$$
\boldsymbol{R} = \boldsymbol{S}^{-1}\boldsymbol{\Sigma}\boldsymbol{S}^{-1}
$$

where $\boldsymbol{S}$ is diagonal with entries $\sqrt{\Var(X_i)}$. The diagonal of $\boldsymbol{R}$ contains all ones.

# The Linear Model

Suppose we have $i=1,\ldots,n$ continuous outcomes $Y_i$ with
covariates $\boldsymbol{X}_i$, a vector of length $p$. To simplify
notation, the first entry of $\boldsymbol{X}_i$ is assumed to be
always 1. The linear model relates the covariates $\boldsymbol{X}_i$
to the outcomes $Y_i$ in a linear fashion.  This is most easily
described if we stack all outcomes into a vector $\boldsymbol{Y}$ of
length $n$ and stack the covariates $\boldsymbol{X}_i$ into a matrix
\[
\boldsymbol{X} = \left( \begin{array}{c} \boldsymbol{X}_1^{\top} \\   \boldsymbol{X}_2^{\top} \\ \vdots  \\ \boldsymbol{X}_n^{\top} \end{array} \right). 
\]
Likewise, additional measurement error terms $\epsilon_i$, $i=1,\ldots,n$, are also stacked into a vector $\boldsymbol{\epsilon}$ of length $n$. 

## Standard Formulation
The standard formulation of the linear model assumes, that the error
terms $\epsilon_i$ have mean zero and are mutually independent with common variance
$\sigma^2$. The relationship between $\boldsymbol{X}$ and $\boldsymbol{Y}$ is described by the **regression coefficients** 
$\boldsymbol{\beta}$, a vector of length $p$, and we obtain the standard linear model
\[
\boldsymbol{Y} = \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}.
\]
The **least squares** estimate of $\boldsymbol{\beta}$ is 
\[
\widehat{\boldsymbol{\beta}} = (\boldsymbol{X}^{\top} \boldsymbol{X})^{-1}\boldsymbol{X}^{\top} \boldsymbol{Y}.
(\#eq:betahat)
\]
Note that this estimate this does not require knowledge of $\sigma^2$, but the 
covariance matrix of \@ref(eq:betahat) depends on $\sigma^2$:
$$
\Cov(\widehat{\boldsymbol{\beta}}) = \sigma^2 (\boldsymbol{X}^{\top} \boldsymbol{X})^{-1}.
(\#eq:covBeta)
$$
The variance $\sigma^2$ can be estimated in a second step based on the sum of the squared residuals $e_i = Y_i - \boldsymbol{X}_i^\top \widehat{\boldsymbol{\beta}}$, $\boldsymbol{e}=(e_1, \ldots, e_n)^\top$:
\[
\hat \sigma^2 =  \frac{1}{n-p} \boldsymbol{e}^\top \boldsymbol{e} = \frac{1}{n-p} \left(\boldsymbol{Y} - \boldsymbol{X}\widehat{\boldsymbol{\beta}}\right)^{\top}\left(\boldsymbol{Y} - \boldsymbol{X}\widehat{\boldsymbol{\beta}}\right).
\]
Plugging in $\hat \sigma^2$ into \@ref(eq:covBeta) can be used to obtain
standard errors for the coefficients of $\widehat{\boldsymbol{\beta}}$ taking the square roots of the diagonal elements. 

An example of a linear model is the ANCOVA model discussed in Chapter \@ref(contOut).

## The General Linear Model {#General}
This section introduces **GEE** and **GLS** for the analysis of longitudinal data, which account for correlations between measurements from the same individual.

:::{.definition #balanced}
Longitudinal data is called **balanced**, if the same number of observations $n_i=n$ for each individual $i= 1,...,m$ at the same time points $t_{ij}=t_j, j=1,...,n.$
:::

### GEE's {#GEE}
**Model formulation**
\begin{equation} 
(\#eq:model)
\boldsymbol{Y} \sim \boldsymbol{\Nor}(\boldsymbol{X\beta}, \boldsymbol{\Sigma})
\end{equation}
where $\boldsymbol{\Sigma}$ is block-diagonal with $n \times n$ entries $\boldsymbol{\Sigma_0}$. This means that 
$$
\boldsymbol{\Sigma} =
\begin{pmatrix}
\boldsymbol{\Sigma}_0 & & & \\
 & \boldsymbol{\Sigma}_0 & & \\
 & & \ddots & \\
 & & &\boldsymbol{\Sigma}_0 
\end{pmatrix}
$$
with zero entries elsewhere.

$\widehat{\boldsymbol{\beta}}_{\boldsymbol{W}}$ are the weighted least squares estimate of $\boldsymbol{\beta}$. Given any weight matrix $\boldsymbol{W}$, $\widehat{\boldsymbol{\beta}}_{\boldsymbol{W}}$ is unbiased with $\E(\widehat {\boldsymbol{\beta}}_{\boldsymbol{W}}) = {\boldsymbol{\beta}}$. The weight matrix $\boldsymbol{W}$ is block-diagonal, with each block $\boldsymbol{W_0}$ of size $n \times n$ for $n$ time points. 

The associated covariance matrix is ("sandwich formula"):

\begin{align*}
\Cov(\widehat{\boldsymbol{\beta}}_{\boldsymbol{W}}) 
&= \boldsymbol{V} \, \boldsymbol{\Sigma} \, \boldsymbol{V}^{\top} \\
\mbox{ where } \boldsymbol{V} 
&= (\boldsymbol{X}^{\top}\boldsymbol{W}\boldsymbol{X})^{-1}\boldsymbol{X}^{\top}\boldsymbol{W}
\end{align*}

and requires knowledge of $\boldsymbol{\Sigma}$ (or an estimate $\widehat{\boldsymbol{\Sigma}}$). 

Notice that $\boldsymbol{W}^{-1}$ is often called **working covariance matrix** to distinguish it from the **true covariance matrix** $\boldsymbol{\Sigma}$. It is an approximation to the true covariance matrix of the outcomes. And here are some special cases of $\boldsymbol{W}^{-1}$:

- If $\boldsymbol{W}^{-1} = \boldsymbol{I}$, then we obtain the **least squares estimate** $\widehat {\boldsymbol{\beta}}_{\boldsymbol{W}} = (\boldsymbol{X}^{\top} \boldsymbol{X})^{-1}\boldsymbol{X}^{\top} \boldsymbol{Y}$.
- If $\boldsymbol{W}^{-1} = \boldsymbol{\Sigma}$, then $\Cov(\widehat{\boldsymbol{\beta}}_{\boldsymbol{W}})$ can be simplified to
\[
 \Cov(\widehat{\boldsymbol{\beta}}_{\boldsymbol{W}}) = (\boldsymbol{X}^{\top}\boldsymbol{W}\boldsymbol{X})^{-1}
\]
If we set ${\boldsymbol{\Sigma}}$ to $\boldsymbol{W}^{-1}$ in the sandwich formula for 
$\Cov(\widehat{\boldsymbol{\beta}}_{\boldsymbol{W}})$, we assume that $\Cov (\boldsymbol{Y})= \boldsymbol{W}^{-1}$ (up to scale). This gives us the **naive standard errors** of $\widehat{\boldsymbol{\beta}}_{\boldsymbol{W}}$.

The **sandwich estimate** $\widehat {\boldsymbol R}_{\boldsymbol{W}} = \boldsymbol{V} \, \widehat{\boldsymbol{\Sigma}} \, \boldsymbol{V}^{\top}$ is a consistent estimate of $\Cov(\widehat{\boldsymbol{\beta}}_{\boldsymbol{W}})$, where the estimate $\widehat{\boldsymbol{\Sigma}}_0$ is the **empirical covariance matrix** of the residuals $\boldsymbol{r}_i = {\boldsymbol Y}_i - \boldsymbol{X}_i\widehat {\boldsymbol \beta}_{\boldsymbol{W}}$, and $\widehat{\boldsymbol{\Sigma}}$ is block-diagonal with
entries $\widehat{\boldsymbol{\Sigma}}_0$. The **robust standard errors** can be extracted from the diagonal: $\sqrt{\diag(\widehat {\boldsymbol R}_{\boldsymbol{W}})}$.

If the naive standard errors are close to the robust standard errors, then $\boldsymbol{W}^{-1}$ must be close to $\Cov (\boldsymbol{Y})$ (up to scale).

Generalized estimating equations work in the following way: 

1. assume some **working correlation matrix** $\boldsymbol{W}$
2. estimate the coefficients with the **weighted least squares estimate** $\widehat{\boldsymbol{\beta}}_{\boldsymbol{W}}$
3. estimate the correlation $\widehat{\boldsymbol{\Sigma}}$ of the residuals
4. compute **robust standard errors** (taking residual correlation into account) from the covariance matrix
$$\widehat {\boldsymbol{R}}_{\boldsymbol{W}} = \boldsymbol{V} \, \widehat{\boldsymbol{\Sigma}} \, \boldsymbol{V}^{\top}$$ based on the "sandwich" formula applied to the "naive" covariance matrix $\boldsymbol{V}$ of the weighted least-squares estimate $\widehat{\boldsymbol{\beta}}_{\boldsymbol{W}}$.

### GLS's
Generalized Least Squares (GLS) is a parametric method for estimating regression parameters when errors are correlated. Different from GEE, GLS uses Maximum Likelihood (\(\texttt{ML}\)) or Restricted Maximum Likelihood (\(\texttt{REML}\)). 

**Model formulation**
\begin{equation} 
\boldsymbol{Y} \sim \boldsymbol{\Nor}(\boldsymbol{X\beta}, \boldsymbol{\Sigma})
\end{equation}
where $\Sigma=\sigma^2 \boldsymbol{V}$ is block-diagonal with $n \times n$ entries $\boldsymbol{\Sigma}_0=\sigma^2 \boldsymbol{V}_0$. Here $\boldsymbol{V}_0$ is a **correlation matrix** with entries corresponding to correlations between observations from the same individual. This correlation matrix often only depends on a few parameters.

Below are some commonly used **parametric correlation structures**. 

- **Uniform correlation model, compound symmetry**
\[
\boldsymbol{V}_0 = (1 - \rho) \boldsymbol{I} + \rho \boldsymbol{J},
\]
where $\boldsymbol{I}$ is the identity matrix, $\boldsymbol{J}$ is a matrix of ones
and $\rho \in [0,1)$ is an unknown correlation parameter. This is also the \(\texttt{exchangeable}\) correlation structure.

- **Exponential correlation model**

    Set $\rho_{jk} = \exp(- \phi d_{jk})$ where $d_{jk}=|t_j - t_k|$ is the distance (in some unit) between two time points $t_j$ and $t_k$:
\[
\boldsymbol{V}_0  = \left\{\exp(- \phi d_{jk})\right\}_{j,k}.
\]
The parameter $\phi$ represents the exponential decay of the correlation. 
The inverse $1/\phi$ is often called the **range parameter**: For a distance $d_{jk}>1/\phi$ we have "small" correlation $\rho_{jk} < \exp(-1)= 0.37$. 

- **Continuous time AR-1 model**: 
This is a reparameterisation of the exponential correlation model, where $\rho_{jk} = \left\{\alpha^{d_{jk}}\right\}_{j,k}$ and $\alpha=\exp(-\phi)$. This is also the \(\texttt{AR-1}\) correlation structure mentioned earlier.

To choose which correlation structure to use, we may use the **model choice criteria**
$$
\mbox{AIC} = - 2 l(\boldsymbol{\hat{\theta}_{\text{ML}}}) + 2\tilde p\\
\mbox{BIC} = - 2 l(\boldsymbol{\hat{\theta}_{\text{ML}}}) + \tilde p\log(\tilde n)
$$
where $l(.)$ is the log likelihood, $\hat{\theta}_{\text{ML}}$ is the maximum likelihood estimate, $\tilde p$ is the number of **all** unknown parameters $\boldsymbol{\theta}$ (including the correlation parameters), and $\tilde n$ is the number of all observations. Smaller values of $\mbox{AIC}$ and $\mbox{BIC}$ are preferred.

**ML and REML estimation**

In model \@ref(eq:model), the block entries $\boldsymbol{\Sigma}_0=\sigma^2 \boldsymbol{V}_0$ in $\boldsymbol{\Sigma}=\sigma^2 \boldsymbol{V}$ often only depends on a few parameters $\boldsymbol{\alpha}$ under parametric models, and it has to be estimated with numerical routines. 

For fixed $\boldsymbol{V}_0$ we obtain the ML estimate of $\boldsymbol{\beta}$:
\begin{equation} 
\widehat{\boldsymbol{\beta}} (\boldsymbol{V}_0)= (\boldsymbol{X}^{\top}\boldsymbol{V}^{-1}\boldsymbol{X})^{-1}\boldsymbol{X}^{\top}\boldsymbol{V}^{-1}\boldsymbol{y}, (\#eq:weighted)
\end{equation}
which can be identified as the weighted least-squares estimate with $\boldsymbol{W}=\boldsymbol{V}^{-1}$.

We can also obtain the ML estimate of $\sigma^2$:
\[
\widehat{\sigma}^2 (\boldsymbol{V}_0)= \mbox{RSS}(\boldsymbol{V}_0)/(nm),
\]

\[
 \text{ where } \text{RSS}(\boldsymbol{V}_0) = {(\boldsymbol{y}-\boldsymbol{X}\widehat{\boldsymbol{\beta}})^{\top}\boldsymbol{V}^{-1}(\boldsymbol{y}-\boldsymbol{X}\widehat{\boldsymbol{\beta}})}
\]
are the (weighted) residual sum of squares, where $\widehat{\boldsymbol{\beta}} = \widehat{\boldsymbol{\beta}} (\boldsymbol{V}_0)$ is defined in \@ref(eq:weighted).
 
However, ML estimates of $\sigma^2$ and $\boldsymbol{V}_0$ are typically biased and problematic if the dimension $p$ of $\boldsymbol{\beta}$ is large. For example, in the classical linear model ($\boldsymbol{V}_0=\boldsymbol{I}$) the ML estimate ${\sigma_{\text{ML}}}^2 = \mbox{RSS}/(nm)$ is biased, whereas ${\sigma_{\text{REML}}}^2 = \mbox{RSS}/(nm-p)$ is unbiased. The latter estimate is a **Restricted Maximum Likelihood** (REML) estimate, and is the default in function \(\texttt{gls()}\). Also notice that if the sample size $nm$ is large, then ML and REML estimates are very similar.

## Random Effects Model {#RandEff}
### General formulation
Suppose there are $p$ covariates and we want to consider $q$ of them as random effects, we could include individual-specific random effects $\boldsymbol{U}_i$ into the model:
\begin{equation}
  Y_{ij} \given \boldsymbol{U}_i,{\epsilon}_{ij} = \boldsymbol{x}_{ij}^{\top}\boldsymbol{\beta} + \boldsymbol{d}_{ij}^{\top}\boldsymbol{U}_i + \epsilon_{ij}
\end{equation}
where $\boldsymbol{U}_i$'s are mutually independent and $\boldsymbol{U}_i\sim \Nor_q(\boldsymbol{0}, \boldsymbol{G})$ and $\epsilon_{ij} \sim \Nor(0, \tau^2)$. The vector $\boldsymbol{d}_{ij}$ ($q \times 1$) is known and in general a sub-vector of the covariates $\boldsymbol{x}_{ij}$ ($p \times 1$). Then we obtain a linear **mixed effects** model with **fixed effects** $\boldsymbol{\beta}$ and **random effects** $\boldsymbol{U}_i$.

We might want to consider random effects when we want to capture **individual heterogeneity** in the mean response. Random effect model is useful as it induce **marginal correlation** between observations $Y_{ij}$ and $Y_{ik}$ from the same individual (see equation \@ref(eq:marg2)).

**Parameter estimation**
The marginal moments
\begin{align*}
  \E(\boldsymbol{Y}_i) & = \boldsymbol{X}_i \boldsymbol{\beta} (\#eq:marg1) \\
  \Cov(\boldsymbol{Y}_i) & = \boldsymbol{D}_i \boldsymbol{G}\boldsymbol{D}_i^{\top} + \tau^2 \boldsymbol{I}_{n_i} (\#eq:marg2)
\end{align*}
determine the (normal) likelihood to estimate $\boldsymbol{\beta}$, $\boldsymbol{G}$ and $\tau^2$ via (RE)ML. 
Here $\boldsymbol{D}_i = (\boldsymbol{d}_{i,1},\ldots,\boldsymbol{d}_{i,n_i})^{\top}$ ($n_i \times q$).

It is also possible to estimate ("predict") the random effects $\boldsymbol{U}_i$ and to compute standard errors, and this is done with the **empirical Bayes (EB)** estimate (the **empirical BLUP** estimate).

### Random intercept model {#A:randInt}
The random intercept model is a special case of a random effects model. In this case, $q=1$ and ${d}_{ij}=1$. 

With $\boldsymbol{x}_{ij}^{\top} = (1, \tilde{\boldsymbol{x}}_{ij}^{\top})$ we have
$$
  Y_{ij}\given {U}_i,{\epsilon}_{ij} = (1, \tilde{\boldsymbol{x}}_{ij}^{\top}) \cdot \boldsymbol{\beta} + {U}_i + \epsilon_{ij}.
$$

This can be rewritten as
$$
    Y_{ij}\given {\tilde U}_i,{\epsilon}_{ij}  =  {\tilde U}_i + \tilde{\boldsymbol{x}}_{ij}^{\top}\tilde{\boldsymbol{\beta}} + \epsilon_{ij}
$$
where ${\tilde U}_i \sim \Nor(\beta_0, \nu^2)$ represents the **random intercept**, varying between individuals with mean $\beta_0$ and variance $\nu^2$.

### Random slope model {#randSlope}
Suppose observations $Y_{ij}$ are made at time points $t_{ij}$, which are included in the covariate vector $\boldsymbol{x}_{ij}=(1,t_{ij},\tilde{\boldsymbol{x}}_{ij}^{\top})^{\top}$.

Suppose $q=2$ and $\boldsymbol{d}_{ij}=(1, t_{ij})^{\top}$, then the model becomes
$$
  Y_{ij}\given \boldsymbol{U}_i,{\epsilon}_{ij} = (1, t_{ij},\tilde{\boldsymbol{x}}_{ij}^{\top}) \cdot \boldsymbol{\beta} + (1, t_{ij}) \cdot \boldsymbol{U}_i + \epsilon_{ij}
$$
where $\boldsymbol{U}_i$ is a 2-dimensional random effect with mean zero and $2 \times 2$ symmetric covariance matrix $\boldsymbol{G}$.
This is the **random slope** (strictly random intercept and slope) model.

# The Generalized Linear Model
## The General Formulations
Here we provide a brief review of the linear regression model:
$$
Y_i = \boldsymbol{x}_i^{\top} \boldsymbol{\beta} + \epsilon_i \, \mbox{ with } \, \epsilon_i \sim \Nor(0, \sigma^2)
$$
with explanatory variables $\boldsymbol{x}_i$ and regression coefficients $\boldsymbol{ \beta}$ is rewritten as $$Y_i \sim
\Nor(\mu_i=\boldsymbol{ x}_i^{\top} \boldsymbol{ \beta}, \sigma^2)$$ with mean $\mu_i$ and variance $\sigma^2$.

This can be generalized in two ways: firstly we can consider other distributions than normal distribution for the response variable $Y_i$ (for example, Bernoulli, Binomial, Poisson, $\ldots$). Secondly, the mean $\E(Y_i)=\mu_i$ will be linked to the **linear predictor** $\eta_i = \boldsymbol{ x}_i^{\top} \boldsymbol{ \beta}$ using a (monotone) **link function** $g = h^{-1}$:
$$
 g(\mu_i) = \eta_i \, \mbox{ so } \, \mu_i = h(\eta_i).
$$
Notice how this is different from **general** linear models (Appendix \@ref(General)): **general** linear models assume a normal distribution of residuals, while **generalized** linear models relax this assumption and allow for other distributions from the exponential family. A general linear model can be viewed as a  **generalized** linear model with normal errors and identity link.

### Logistic regression
Instead of the normality assumption on $Y_i$ in linear models, other distributions of $Y_i$ are allowed for generalization. 

For logistic regression, consider 
\begin{align*}
Y_i &\sim \Bin(1, \mu_i=\pi_i)\\
\logit(\pi_i)  &=  \boldsymbol{x}_i^{\top} \boldsymbol{\beta}\\
 \pi_i &= \text{expit}(\boldsymbol{ x}_i^{\top} \boldsymbol{ \beta}).
\end{align*}
So here the link function is 
\[
g(\pi_i) = \logit(\pi_i) = \log \frac{\pi_i}{1-\pi_i}.
\]

Except for the intercept, we could interpret $\exp(\beta_i)$ as **odds ratio** in this model. Also notice that the variance is a function of the mean: $\Var(Y_i)=\pi_i(1-\pi_i)$.

If covariates $x_i$ are identical for some observations $Y_i$, then 
aggregation to the binomial form $Y_j = \sum_{i} Y_i \sim \Bin(n_i, \pi_i)$ is possible. 

An example of a logistic regression can be found in Chapter \@ref(logisticRand).

### Log-linear Poisson regression
In addition to binary data, we may consider count data, for which the random variable $Y_i$ follows a Poisson distribution.

Poisson regression assumes
\begin{align*}
Y_i &\sim \Po(\mu_i)\\
\log(\mu_i) &= \boldsymbol{x}_i^{\top} \boldsymbol{\beta} ,\\
\mu_i &= \exp( \boldsymbol{x}_i^{\top} \boldsymbol{\beta} ),\\
\end{align*}
so the link function in this case is
$$g(\mu_i) = \log(\mu_i).$$
As a consequence, $\exp(\beta_k)$ can be interpreted (for all $k \ge 1$) as a multiplicative rate ratio for the expected count. It is also important to note that the variance increases with the mean in this model: $\Var(Y_i)=\E[Y_i]=\mu_i$.

If several observations share identical covariate values $\boldsymbol{x}_i$, then they can be combined into a single grouped Poisson response with outcome $Y_j = \sum_i Y_i$ and corresponding mean $\mu_j = \sum_i \mu_i$.


## GEE's {#GEEs}
This marginal approach is a multivariate quasi-likelihood approach. Quasi-likelihood is used to deal with overdispersion, which means that the variance of the outcome is greater than what is assumed by the model.

<!--GEE intro-->
Consider the weighted least-squares estimate $\hat{\boldsymbol{\beta}}_{\boldsymbol{{W}}}$ in the general linear model with block-diagonal
**working correlation matrix** $\boldsymbol{W}^{-1}$ with entries $\boldsymbol{W}_0^{-1}$
\begin{align*}
\hat{\boldsymbol{\beta}}_{\boldsymbol{{W}}} & = 
(\boldsymbol{X}^{\top}\boldsymbol{W}\boldsymbol{X})^{-1}\boldsymbol{X}^{\top}\boldsymbol{W}\boldsymbol{Y} \\ 
& =  \left(
\sum_{i=1}^m \boldsymbol{X}_i^{\top}\boldsymbol{W}_0 \boldsymbol{X}_i \right)^{-1} \sum_{i=1}^m
\boldsymbol{X}_i^{\top}\boldsymbol{W}_0 \boldsymbol{Y}_i.
\end{align*}
$\hat{\boldsymbol{\beta}}_{\boldsymbol{W}}$ can be seen as solution of the multivariate **score equation**
\begin{equation} 
\boldsymbol{S}(\boldsymbol{\beta}) = \sum_{i=1}^m \left(\frac{\partial \boldsymbol{\mu}_i}{\partial
\boldsymbol{\beta}}\right) \boldsymbol{W}_0 (\boldsymbol{Y}_i - \boldsymbol{\mu}_i)=\boldsymbol{0}, (\#eq:score)
\end{equation}
where $\boldsymbol{\mu}_i=\E(\boldsymbol{Y}_i)=\boldsymbol{X}_i \boldsymbol{\beta}$ and therefore $\left(\frac{\partial \boldsymbol{\mu}_i}{\partial\boldsymbol{\beta}}\right)=\boldsymbol{X}_i^{\top}$.

The score function \@ref(eq:score) differs from the score equation in the ordinary generalized linear model 
$$
{S}(\boldsymbol{\beta}) = \sum_{i=1}^m \left(\frac{\partial {\mu}_i}{\partial
\boldsymbol{\beta}}\right) \Var(Y_i)^{-1} (Y_i - \mu_i) = {0}
$$
by replacing $\Var(Y_i)^{-1}$ with a weight $\boldsymbol{W}_0$.

The basic idea is to use multivariate quasi-likelihood approach for non-normal data to take into account correlation between components of $Y_i$. 

The **generalised estimating equation**
\[
\boldsymbol{S}(\boldsymbol{\beta}) = \sum_{i=1}^m \left(\frac{\partial \boldsymbol{\mu}_i}{\partial
\boldsymbol{\beta}}\right) \Cov(\boldsymbol{Y}_i)^{-1} (\boldsymbol{Y}_i - \boldsymbol{\mu}_i)=\boldsymbol{0}
\]
contains the multivariate $\Cov(\boldsymbol{Y}_i)^{-1}$ to allow for dependence. $\Cov(\boldsymbol{Y}_i)^{-1}$ can be replaced with some **working correlation matrix** $\boldsymbol{W}^{-1}$. This approach is called quasi-likelihood because typically no true likelihood function exists with $\boldsymbol{S}(\boldsymbol{\beta})$ as score equation.

It is possible to obtain robust standard errors using the sandwich formula. The estimates $\hat{\boldsymbol{\beta}}$ (and $\hat{\boldsymbol{ \alpha}}$) are asymptotically normal. 

$\hat{\boldsymbol{\beta}}$ is **consistent** (for $m \rightarrow \infty$), even if the correlation structure is not correct, which means $\Corr(\boldsymbol{Y}_i) \neq \boldsymbol{W}^{-1}$, while the estimates of $\boldsymbol{\beta}$ are **efficient** if $\Corr(\boldsymbol{Y}_i) \approx \boldsymbol{W}^{-1}$.

**Bounds of correlation between binary variables**

Here we discuss the bounds of correlation between binary variable. Consider two binary variables $Y_1$ and $Y_2$ with success probabilities $\pi_1$ and $\pi_2$, say. One can show 
$$\max(0,\pi_1+\pi_2-1) \leq \P(Y_1=1,Y_2=1) \leq \min(\pi_1,\pi_2).$$
This implies certain constraints on the correlation
\begin{align*}
\rho(Y_1,Y_2) 
&= \frac{\E(Y_1 Y_2) - \E(Y_1) \E(Y_2)}{\{\Var(Y_1)\Var(Y_2)\}^{1/2}} \\ 
&=\frac{\P(Y_1=1,Y_2=1) - \pi_1 \pi_2}{\{\pi_1(1-\pi_1)\pi_2(1-\pi_2)\}^{1/2}}.
\end{align*}

For example, suppose $\pi_1=0.5$ and $\pi_2=0.9$, then the correlation between $Y_1$ and $Y_2$ cannot
be larger than 1/3 in absolute value: 
```{r, echo=TRUE}
p1 <- 0.5
p2 <- 0.9
(lim.joint <- c(max(c(0, p1+p2-1)), min(c(p1, p2))))
(lim.corr <- (lim.joint - p1*p2) / sqrt(p1*(1-p1)*p2*(1-p2)))
```

Therefore, correlation as a measure of association may not be optimal for binary variables. Alternatively, we can use the **marginal odds ratio**
$$\OR(Y_1,Y_2) = \frac{\P(Y_1=1,Y_2=1)\P(Y_1=0,Y_2=0)}{\P(Y_1=1,Y_2=0)\P(Y_1=0,Y_2=1)}.$$


## GLM's with Random Effects {#GLMrand}

Condition on random effects $\boldsymbol{U}_i \sim \Nor_q(\boldsymbol{0}, \boldsymbol{G})$ , $Y_{ij}$ follows a generalized linear model with mean $$\mu_{ij} = \E(Y_{ij}) = h(\eta_{ij})$$ and linear predictor 
$$ \eta_{ij}=\boldsymbol{x}_{ij}^T\boldsymbol{\beta}^* + \boldsymbol{d}_{ij}^T\boldsymbol{U}_i,$$
where $\boldsymbol{\beta}^*$ are the conditional coefficients. In this conditional model, parameters estimation is based on the marginal distribution of $\boldsymbol{Y}_i$, which can be obtained through numerical integration.

### Logistic regression model {#logisticRand}
Consider a logistic regression model with random intercept
$$\logit \P(Y_{ij}=1\given U_i) = \boldsymbol{x}_{ij}^T\boldsymbol{\beta}^* + U_i, \quad U_i \sim \Nor(0, \nu^2).$$
In a marginal model, we directly specify $$\logit \P(Y_{ij}=1) = \boldsymbol{x}_{ij}^T\boldsymbol{\beta}.$$
It can be shown that 
\begin{equation} 
\boldsymbol{\beta} \approx \underbrace{(c^2 \nu^2 + 1)^{-1/2}}_{q(\nu^2)} \boldsymbol{\beta}^*, \mbox{ where } c 
\approx 10/17. (\#eq:condmarg)
\end{equation}
Therefore, marginal coefficients $\boldsymbol{\beta}$ and conditional coefficients $\boldsymbol{\beta}^*$ have the same sign with $|\boldsymbol{\beta}| \leq |\boldsymbol{\beta}^*|$. Moreover, their ratio $q(\nu^2)$ decreases with increasing variance $\nu^2$, as shown in Table \@ref(tab:qtable).

```{r qtable, echo=FALSE}
# Define vectors
nu_sq <- c(0, 0.1, 0.5, 1.0, 2.0, 5.0)
q <- c(1.23, 1.45, 2.01, 2.34, 2.89, 3.12)

# Combine into matrix with row names
tbl <- rbind("$$\\nu^2$$" = nu_sq, "$$q(\\nu^2)$$" = q)

# Create table with caption
knitr::kable(tbl, caption = "Values of $q(\\nu^2)$ for different $\\nu^2$.", escape = FALSE)
```


```{r ,echo=FALSE, eval=FALSE}
x <- c(-50:70)/10
beta0  <- -2
beta1 <- 1
nu2 <- 5

n <- 200

invlogit <- function(x){

return(exp(x)/(1+exp(x)))
}
individual <- matrix(ncol=n, nrow=length(x))
set.seed(1234567)
for(i in 1:n){
U <- rnorm(1, 0, sqrt(nu2))
individual[,i] <- invlogit(U+beta0+beta1*x)
}

logit <- function(x) log(x/(1-x))
c <- 16*sqrt(3)/(15*pi)
factor <- (c*c*nu2+1)^{-0.5}
approx <- (factor*(beta0+beta1*x))
marginal <- apply(individual, MARGIN=1, FUN=mean)
par(mfrow=c(1,2), las=1)
matplot(x, cbind(individual), type="l", lty=1, ylab="Probability", xlab="x", lwd=1, col="grey", cex=0.7)
lines(x, marginal, col=2, lwd=3)
#lines(x, invlogit(approx), col=2, lwd=3, lty=2)
legend("topleft", legend=c("individual profiles", "population average"), 
       col=c("grey", "red"), lwd=c(1,3), lty=c(1,1), bg="white")
title("Probability scale")
#legend("topleft", legend=c("20 individual probability profiles", "population average"), col=c("grey","red"), lwd=c(1,3))
matplot(x, logit(cbind(individual)), type="l", lty=1, ylab="Logit Probability", xlab="x", lwd=1, col="grey", xlim=c(-5,5), ylim=c(-7,3), cex=0.7)
lines(x, logit(marginal), col=2, lwd=3)
lines(x, approx, col="darkred", lwd=3, lty=2)
legend("topleft", legend=c("population average", "linear approximation"), col=c("red", "darkred"), lwd=c(3), lty=c(1,2), bg="white")
title("Logit scale")

```

